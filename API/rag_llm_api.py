#!/usr/bin/env python3
"""
RAG + LLM API Service

A standalone API service that exposes the RAG + LLM functionality from the integrated pipeline.
Provides streaming text responses with END_FLAG for completion indication.

Input: text_user_msg
Output: streaming llm_text_response with END_FLAG when done
"""

import os
os.environ["ANONYMIZED_TELEMETRY"] = "False"
os.environ["CHROMA_TELEMETRY_DISABLED"] = "1"
os.environ["POSTHOG_DISABLED"] = "1"
import sys
import json
import time
import logging
import requests
import re
from typing import List, Dict, Any, Optional
from flask import Flask, request, jsonify, Response, stream_template, stream_with_context
from flask_cors import CORS
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# Add parent directories to path to import RAG components
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)
from config import LLM_MODEL_NAME, CHROMA_DB_PATH

sys.path.insert(0, os.path.join(parent_dir, 'LLM_Chat'))
from LLM_Chat.RAG_LLM_realtime import ImprovedRAGPipeline

# Import tone system prompts
from tone_system_prompts_no_tag import get_tone_system_prompt, build_tone_selector_system_prompt, PERCENTAGE, build_fixed_system_prompt, build_query_rewriter_prompt
# from tone_system_prompts import get_tone_system_prompt, build_tone_selector_system_prompt

# Colors for console output
YELLOW = "\033[93m"
GREEN = "\033[92m"
BLUE = "\033[94m"
RED = "\033[91m"
GRAY = "\033[90m"
RESET = "\033[0m"

class RAGLLMAPIService:
    """Standalone API service for RAG + LLM functionality"""
    
    def __init__(self, user_description_server_url: str = "http://localhost:5004"):
        self.app = Flask(__name__)
        CORS(self.app)  # Enable CORS for cross-origin requests
        
        # Setup logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # Initialize RAG pipeline
        self.rag_pipeline = None
        self.chroma_collection = None
        self.rag_initialized = False
        
        # Chat history storage (simple in-memory, can be enhanced with persistent storage)
        self.chat_sessions = {}  # session_id -> chat_history
        
        # Vision Context API configuration
        self.user_description_server_url = user_description_server_url
        
        self._setup_routes()
        
    def _setup_routes(self):
        """Setup Flask routes"""
        
        @self.app.route('/health', methods=['GET'])
        def health_check():
            """Health check endpoint"""
            return jsonify({
                'status': 'healthy',
                'rag_initialized': self.rag_initialized,
                'timestamp': time.time()
            })
        
        @self.app.route('/api/rag-llm/query', methods=['POST'])
        def rag_llm_query():
            """
            Main RAG + LLM query endpoint with streaming response
            
            Input JSON:
            {
                "text_user_msg": "Your question here",
                "session_id": "optional_session_id",
                "include_history": true/false (optional, default: true),
                "user_description": "visual description from VLM (e.g., 'a young boy wearing glasses, and is smiling')",
                "tone": "casual_friendly" (optional, deprecated - use user_description instead),
                "convert_tone": true/false (optional, default: true)
            }
            
            Output: Streaming text response with END_FLAG
            """
            try:
                # Parse request
                data = request.get_json()
                if not data:
                    return jsonify({'error': 'No JSON data provided'}), 400
                
                text_user_msg = data.get('text_user_msg')
                if not text_user_msg:
                    return jsonify({'error': 'text_user_msg is required'}), 400
                
                session_id = data.get('session_id', 'default')
                include_history = data.get('include_history', True)
                user_description = data.get('user_description', '')  # VLM visual description for dynamic tone selection
                tone = data.get('tone', 'casual_friendly')  # Deprecated but kept for backward compatibility
                convert_tone = data.get('convert_tone', False)  # Updated default to True
                
                # Get chat history for this session
                chat_history = self.chat_sessions.get(session_id, []) if include_history else []
                
                # Generate streaming response (with optional tone conversion)
                if convert_tone:
                    return Response(
                        stream_with_context(self._generate_streaming_response_with_tone(
                            text_user_msg, session_id, chat_history, user_description, convert_tone
                        )),
                        mimetype='text/plain',
                        headers={
                            'Cache-Control': 'no-cache',
                            'Connection': 'keep-alive',
                            'X-Accel-Buffering': 'no'  # Disable nginx buffering if present
                        }
                    )
                else:
                    return Response(
                        stream_with_context(self._generate_streaming_response(
                            text_user_msg, session_id, chat_history
                        )),
                        mimetype='text/plain',
                        headers={
                            'Cache-Control': 'no-cache',
                            'Connection': 'keep-alive',
                            'X-Accel-Buffering': 'no'  # Disable nginx buffering if present
                        }
                    )
                
            except Exception as e:
                self.logger.error(f"Query error: {e}")
                return jsonify({'error': str(e)}), 500
        
        @self.app.route('/api/rag-llm/init', methods=['POST'])
        def initialize_rag():
            """Initialize the RAG system"""
            try:
                success = self._initialize_rag_system()
                
                return jsonify({
                    'success': success,
                    'rag_initialized': self.rag_initialized,
                    'message': 'RAG system initialized successfully' if success else 'RAG initialization failed'
                })
            except Exception as e:
                self.logger.error(f"RAG initialization error: {e}")
                return jsonify({'error': str(e)}), 500
        
        @self.app.route('/api/rag-llm/sessions/<session_id>/history', methods=['GET', 'DELETE'])
        def manage_session_history(session_id):
            """Get or clear chat history for a session"""
            if request.method == 'GET':
                history = self.chat_sessions.get(session_id, [])
                return jsonify({
                    'session_id': session_id,
                    'history': history,
                    'message_count': len(history)
                })
            elif request.method == 'DELETE':
                self.chat_sessions.pop(session_id, None)
                return jsonify({
                    'session_id': session_id,
                    'message': 'History cleared'
                })
        
        @self.app.route('/api/rag-llm/close', methods=['POST'])
        def close_connection():
            """
            Elegant connection close endpoint
            
            Client calls this function before program termination.
            Server will clear the history of the client based on the chat_session_id.
            
            Input JSON:
            {
                "session_id": "your_session_id"
            }
            
            Output: Confirmation of successful closure and cleanup
            """
            try:
                # Parse request
                data = request.get_json()
                if not data:
                    return jsonify({'error': 'No JSON data provided'}), 400
                
                session_id = data.get('session_id')
                if not session_id:
                    return jsonify({'error': 'session_id is required'}), 400
                
                # Perform cleanup operations
                session_existed = session_id in self.chat_sessions
                message_count = len(self.chat_sessions.get(session_id, []))
                
                # Clear session history
                self.chat_sessions.pop(session_id, None)
                
                # Log the closure
                self.logger.info(f"Connection closed gracefully for session: {session_id}")
                print(f"{GREEN}ðŸ‘‹ Session {session_id} closed gracefully ({message_count} messages cleared){RESET}")
                
                return jsonify({
                    'success': True,
                    'session_id': session_id,
                    'message': 'Connection closed successfully',
                    'session_existed': session_existed,
                    'messages_cleared': message_count,
                    'timestamp': time.time()
                })
                
            except Exception as e:
                self.logger.error(f"Connection close error: {e}")
                return jsonify({'error': str(e)}), 500
        
        @self.app.route('/api/rag-llm/warmup', methods=['POST'])
        def warmup_models():
            """
            Warmup endpoint to preload embedding model and LLM
            
            This endpoint performs small test operations to ensure both the embedding
            model and LLM are loaded into memory, reducing latency on first real requests.
            
            Output: Status of warmup operations for both models
            """
            try:
                print(f"{BLUE}ðŸ”¥ Starting model warmup...{RESET}")
                
                warmup_results = {
                    'embedding_model': {'status': 'skipped', 'message': 'RAG not initialized', 'time_ms': 0},
                    'llm_model': {'status': 'failed', 'message': 'Not attempted', 'time_ms': 0},
                    'overall_success': False,
                    'timestamp': time.time()
                }
                
                # Warmup embedding model
                embedding_result = self._warmup_embedding_model()
                warmup_results['embedding_model'] = embedding_result
                
                # Warmup LLM
                llm_result = self._warmup_llm_model()
                warmup_results['llm_model'] = llm_result
                
                # Determine overall success
                # LLM success is critical, embedding can be skipped due to dimension mismatch
                warmup_results['overall_success'] = (
                    embedding_result['status'] in ['success', 'skipped'] and 
                    llm_result['status'] == 'success'
                )
                
                total_time = embedding_result['time_ms'] + llm_result['time_ms']
                print(f"{GREEN}ðŸ”¥ Model warmup completed in {total_time:.0f}ms{RESET}")
                
                return jsonify(warmup_results)
                
            except Exception as e:
                self.logger.error(f"Warmup error: {e}")
                return jsonify({
                    'embedding_model': {'status': 'failed', 'message': str(e), 'time_ms': 0},
                    'llm_model': {'status': 'failed', 'message': 'Warmup failed', 'time_ms': 0},
                    'overall_success': False,
                    'timestamp': time.time(),
                    'error': str(e)
                }), 500

        @self.app.route('/api/rag-llm/convert-tone', methods=['POST'])
        def convert_tone():
            """
            Standalone tone conversion endpoint with streaming response
            
            Input JSON:
            {
                "text": "Text to convert",
                "tone": "child_friendly" (optional, default),
                "stream": true/false (optional, default: true),
                "user_description": "visual description from VLM" (optional),
                "user_msg": "original user message" (optional)
            }
            
            Output: Streaming tone-converted text with END_FLAG
            """
            try:
                # Parse request
                data = request.get_json()
                if not data:
                    return jsonify({'error': 'No JSON data provided'}), 400
                
                text = data.get('text')
                if not text:
                    return jsonify({'error': 'text is required'}), 400
                
                tone = data.get('tone', 'child_friendly')
                use_streaming = data.get('stream', True)
                user_description = data.get('user_description', '')
                user_msg = data.get('user_msg', '')
                
                if use_streaming:
                    # Generate streaming response
                    return Response(
                        stream_with_context(self._stream_convert_tone(text, tone, user_description, user_msg)),
                        mimetype='text/plain',
                        headers={
                            'Cache-Control': 'no-cache',
                            'Connection': 'keep-alive',
                            'X-Accel-Buffering': 'no'  # Disable nginx buffering if present
                        }
                    )
                else:
                    # Non-streaming response
                    converted_text = self._convert_tone(text, tone, user_description, user_msg)
                    if converted_text is not None:
                        return jsonify({
                            'success': True,
                            'original_text': text,
                            'converted_text': converted_text,
                            'tone': tone,
                            'user_description': user_description
                        })
                    else:
                        return jsonify({'error': 'Tone conversion failed'}), 500
                
            except Exception as e:
                self.logger.error(f"Tone conversion error: {e}")
                return jsonify({'error': str(e)}), 500

        @self.app.route('/api/rag-llm/query-with-tone', methods=['POST'])
        def rag_llm_query_with_tone():
            """
            Combined RAG + LLM query with automatic tone conversion
            
            Input JSON:
            {
                "text_user_msg": "Your question here",
                "session_id": "optional_session_id",
                "include_history": true/false (optional, default: true),
                "user_description": "visual description from VLM (e.g., 'a young boy wearing glasses, and is smiling')",
                "tone": "casual_friendly" (optional, deprecated - use user_description instead),
                "convert_tone": true/false (optional, default: true)
            }
            
            Output: Streaming response with automatic tone conversion and END_FLAG
            """
            try:
                # Parse request
                data = request.get_json()
                if not data:
                    return jsonify({'error': 'No JSON data provided'}), 400
                
                text_user_msg = data.get('text_user_msg')
                if not text_user_msg:
                    return jsonify({'error': 'text_user_msg is required'}), 400
                
                session_id = data.get('session_id', 'default')
                include_history = data.get('include_history', True)
                user_description = data.get('user_description', '')  # VLM visual description for dynamic tone selection
                tone = data.get('tone', 'casual_friendly')  # Deprecated but kept for backward compatibility
                convert_tone = data.get('convert_tone', True)
                
                # Get chat history for this session
                chat_history = self.chat_sessions.get(session_id, []) if include_history else []
                
                # Generate streaming response with tone conversion
                return Response(
                    stream_with_context(self._generate_streaming_response_with_tone(
                        text_user_msg, session_id, chat_history, user_description, convert_tone
                    )),
                    mimetype='text/plain',
                    headers={
                        'Cache-Control': 'no-cache',
                        'Connection': 'keep-alive',
                        'X-Accel-Buffering': 'no'  # Disable nginx buffering if present
                    }
                )
                
            except Exception as e:
                self.logger.error(f"Query with tone error: {e}")
                return jsonify({'error': str(e)}), 500
    
    def _initialize_rag_system(self) -> bool:
        """Initialize the RAG system with ChromaDB path detection"""
        try:
            print(f"{BLUE}ðŸ”„ Initializing RAG system...{RESET}")
            
            # Initialize RAG pipeline
            if not self.rag_pipeline:
                self.rag_pipeline = ImprovedRAGPipeline()
                print(f"{GREEN}âœ… RAG pipeline created{RESET}")
            
            # Try multiple ChromaDB paths
            potential_paths = [
                f"{CHROMA_DB_PATH}",  # User's actual ChromaDB location
            ]
            
            chroma_client = None
            chroma_db_path = None
            
            import chromadb
            
            # Try to find existing ChromaDB
            for path in potential_paths:
                if os.path.exists(path):
                    try:
                        print(f"{BLUE}ðŸ” Trying ChromaDB path: {path}{RESET}")
                        chroma_client = chromadb.PersistentClient(path=path)
                        chroma_db_path = path
                        break
                    except Exception as e:
                        print(f"{YELLOW}âš ï¸ Failed to connect to {path}: {e}{RESET}")
                        continue
            
            # If no existing ChromaDB found, create new one in workspace
            if not chroma_client:
                print(f"{RED}âŒ No ChromaDB found{RESET}")
                # Create new ChromaDB in the user's specified location
                chroma_db_path = f"{CHROMA_DB_PATH}"
                print(f"{BLUE}ðŸ“¦ Creating new ChromaDB at: {chroma_db_path}{RESET}")
                os.makedirs(chroma_db_path, exist_ok=True)
                chroma_client = chromadb.PersistentClient(path=chroma_db_path)
            
            print(f"{GREEN}âœ… Using ChromaDB at: {chroma_db_path}{RESET}")
            
            # Try different collection names
            collection_names = [
                f"{self.rag_pipeline.museum_name}_collection",  # itri_museum_collection
                "itri_museum_docs",  # From error message
                f"{self.rag_pipeline.museum_name}_docs",  # itri_museum_docs
                "museum_collection",  # Generic name
                "default_collection"  # Fallback
            ]
            
            # List existing collections to see what's available
            try:
                existing_collections = chroma_client.list_collections()
                if existing_collections:
                    print(f"{BLUE}ðŸ“‹ Available collections:{RESET}")
                    for coll in existing_collections:
                        print(f"  - {coll.name} ({coll.count()} docs)")
                        collection_names.insert(0, coll.name)  # Prioritize existing collections
                else:
                    print(f"{YELLOW}ðŸ“‹ No existing collections found{RESET}")
            except Exception as e:
                print(f"{YELLOW}âš ï¸ Could not list collections: {e}{RESET}")
            
            # Try to load or create collection
            collection_found = False
            for collection_name in collection_names:
                try:
                    self.chroma_collection = chroma_client.get_collection(collection_name)
                    count = self.chroma_collection.count()
                    print(f"{GREEN}âœ… Loaded existing collection: {collection_name} ({count} documents){RESET}")
                    collection_found = True
                    break
                except Exception:
                    continue
            
            # If no collection found, create a new one with sample data
            if not collection_found:
                print(f"{YELLOW}ðŸ†• Creating new collection with sample data...{RESET}")
                try:
                    chunks = self.rag_pipeline.load_json_data()
                    if chunks:
                        collection_name = f"{self.rag_pipeline.museum_name}_collection"
                        self.chroma_collection = self.rag_pipeline.build_vector_database(
                            chunks, chroma_client, collection_name
                        )
                        print(f"{GREEN}âœ… Created new collection: {collection_name} ({len(chunks)} chunks){RESET}")
                    else:
                        # Create minimal collection for testing (without embeddings for now)
                        collection_name = f"{self.rag_pipeline.museum_name}_collection"
                        try:
                            # Try with default embedding function first
                            self.chroma_collection = chroma_client.create_collection(
                                name=collection_name,
                                metadata={"description": "ITRI Museum collection"}
                            )
                        except Exception as embedding_error:
                            print(f"{YELLOW}âš ï¸ Default embedding failed: {embedding_error}{RESET}")
                            # Try with simpler embedding function
                            try:
                                import chromadb.utils.embedding_functions as embedding_functions
                                # Use sentence transformer without downloading
                                self.chroma_collection = chroma_client.create_collection(
                                    name=collection_name,
                                    embedding_function=embedding_functions.DefaultEmbeddingFunction(),
                                    metadata={"description": "ITRI Museum collection"}
                                )
                            except Exception as e2:
                                print(f"{YELLOW}âš ï¸ Embedding function error: {e2}{RESET}")
                                # Final fallback - just create without specific embedding
                                self.chroma_collection = chroma_client.get_or_create_collection(
                                    name=collection_name,
                                    metadata={"description": "ITRI Museum collection"}
                                )
                        
                        # Add sample documents
                        try:
                            sample_docs = [
                                "ITRI (Industrial Technology Research Institute) is Taiwan's largest and most comprehensive research institution.",
                                "ITRI focuses on applied research in areas such as information and communications, electronics, materials, chemical engineering, and biomedical technologies.",
                                "Founded in 1973, ITRI has been instrumental in Taiwan's technological development and industrial transformation."
                            ]
                            sample_ids = ["sample_1", "sample_2", "sample_3"]
                            sample_metadatas = [
                                {"source": "general", "type": "overview"},
                                {"source": "general", "type": "research_areas"},
                                {"source": "general", "type": "history"}
                            ]
                            
                            self.chroma_collection.add(
                                documents=sample_docs, 
                                ids=sample_ids,
                                metadatas=sample_metadatas
                            )
                            print(f"{GREEN}âœ… Created minimal collection: {collection_name} ({len(sample_docs)} sample documents){RESET}")
                        except Exception as add_error:
                            print(f"{YELLOW}âš ï¸ Could not add sample documents: {add_error}{RESET}")
                            print(f"{GREEN}âœ… Created empty collection: {collection_name}{RESET}")
                            
                except Exception as e:
                    print(f"{YELLOW}âš ï¸ Collection creation had issues: {e}{RESET}")
                    # Try to continue anyway - maybe we can work without RAG
                    print(f"{BLUE}ðŸ”„ Attempting to continue without full RAG capabilities...{RESET}")
                    try:
                        # Create a dummy collection just to keep the service working
                        collection_name = "fallback_collection"
                        self.chroma_collection = chroma_client.get_or_create_collection(
                            name=collection_name,
                            metadata={"description": "Fallback collection"}
                        )
                        print(f"{YELLOW}âš ï¸ Using fallback collection - RAG features may be limited{RESET}")
                    except Exception as fallback_error:
                        print(f"{RED}âŒ Could not create fallback collection: {fallback_error}{RESET}")
                        # Continue without ChromaDB - LLM only mode
                        self.chroma_collection = None
                        print(f"{YELLOW}âš ï¸ Running in LLM-only mode without RAG{RESET}")
                        return True  # Still consider this successful
            
            # Build cached system prompt
            self.rag_pipeline.cached_system_prompt = build_fixed_system_prompt(
                "NOTICE: The response language should depend on what the user requires. If the user does not specify a language, use the same language as the user input. If Mandarin or Chinese is requested, respond in Traditional Chinese (zh-tw). Most importantly, ensure all information comes from rag_reference and is accurate and truthful."
            )
            
            self.rag_initialized = True
            print(f"{GREEN}âœ… RAG system initialized successfully{RESET}")
            return True
            
        except Exception as e:
            print(f"{RED}âŒ RAG initialization failed: {e}{RESET}")
            self.rag_initialized = False
            return False
    
    def _rewrite_query(self, user_question: str, chat_history: List[Dict]) -> str:
        """
        Rewrite user question using chat history context to create a better search query.
        
        Args:
            user_question: The current user question
            chat_history: Previous conversation history
        
        Returns:
            str: Rewritten query optimized for embedding search, or original question if rewriting fails
        """
        try:
            # Detect input language
            has_chinese = any('\u4e00' <= char <= '\u9fff' for char in user_question)
            target_lang = "Traditional Chinese (ç¹é«”ä¸­æ–‡)" if has_chinese else "English"
            
            # Build system prompt
            system_prompt = build_query_rewriter_prompt(target_lang)
            
            # Format chat history for context
            history_context = ""
            if chat_history:
                history_context = "\nChat History:\n"
                for msg in chat_history[-4:]:  # Use last 4 messages for context
                    role = msg.get("role", "")
                    content = msg.get("content", "")
                    if role == "user":
                        history_context += f"User: {content}\n"
                    elif role == "assistant":
                        history_context += f"Assistant: {content}\n"
            
            # Build user instruction
            user_instruction = f"""Rewrite this user question into a searchable query optimized for embedding-based retrieval.

{history_context}
Current User Question: {user_question}

Output ONLY the rewritten query without any explanations or prefixes."""

            payload = {
                "model": f"{LLM_MODEL_NAME}",
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_instruction},
                ],
                "stream": False,
                "options": {"temperature": 0.3}  # Lower temperature for more consistent rewriting
            }
            
            resp = requests.post("http://localhost:11435/api/chat", json=payload, timeout=10)
            resp.raise_for_status()
            data = resp.json()
            
            if isinstance(data, dict) and "message" in data and isinstance(data["message"], dict):
                rewritten_query = data["message"].get("content", "").strip()
            elif isinstance(data, dict) and "response" in data:
                rewritten_query = str(data.get("response", "")).strip()
            else:
                rewritten_query = ""
            
            # Clean up the response (remove any prefixes that might have been added)
            rewritten_query = rewritten_query.replace("Rewritten query:", "").replace("Query:", "").strip()
            rewritten_query = rewritten_query.split("\n")[0].strip()  # Take first line only
            
            if rewritten_query and len(rewritten_query) > 3:
                print(f"{BLUE}ðŸ”„ Query rewritten: '{user_question}' â†’ '{rewritten_query}'{RESET}")
                return rewritten_query
            else:
                print(f"{YELLOW}âš ï¸ Query rewriting failed or returned empty, using original query{RESET}")
                return user_question
                
        except Exception as e:
            self.logger.error(f"Query rewriting failed: {e}")
            print(f"{YELLOW}âš ï¸ Query rewriting error: {e}, using original query{RESET}")
            return user_question  # Fallback to original question
    
    def _determine_tone_from_user_description(self, user_description: str) -> str:
        """
        Analyze visual user description from VLM and determine the most appropriate tone.
        
        Args:
            user_description: Visual description from VLM (e.g., "a young boy wearing glasses, and is smiling")
        
        Returns:
            str: The determined tone ('child_friendly', 'elder_friendly', etc.)
        """
        try:
            if not user_description or not user_description.strip():
                return "casual_friendly"  # Default fallback
            
            # Detect input language
            has_chinese = any('\u4e00' <= char <= '\u9fff' for char in user_description)
            target_lang = "Traditional Chinese (ç¹é«”ä¸­æ–‡)" if has_chinese else "English"
            
            # System prompt for VLM-based tone selection agent
            system_prompt = build_tone_selector_system_prompt(target_lang)
            
            # Create user instruction
            user_instruction = f"Analyze this user description and determine the appropriate tone:\n{user_description.strip()}"
            
            payload = {
                "model": f"{LLM_MODEL_NAME}",
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_instruction},
                ],
                "stream": False,
                "options": {"temperature": 0.1}  # Low temperature for consistent analysis
            }
            
            resp = requests.post("http://localhost:11435/api/chat", json=payload, timeout=30)
            resp.raise_for_status()
            data = resp.json()
            
            if isinstance(data, dict) and "message" in data and isinstance(data["message"], dict):
                tone_result = data["message"].get("content", "").strip().lower()
            elif isinstance(data, dict) and "response" in data:
                tone_result = str(data.get("response", "")).strip().lower()
            else:
                tone_result = ""
            
            # Validate and return the tone
            valid_tones = ["child_friendly", "elder_friendly", "professional_friendly", "casual_friendly"]
            if tone_result in valid_tones:
                print(f"{BLUE}ðŸŽ¯ Tone selected: {tone_result} (based on VLM description: '{user_description}'){RESET}")
                return tone_result
            else:
                print(f"{YELLOW}âš ï¸ Invalid tone '{tone_result}', defaulting to casual_friendly{RESET}")
                return "casual_friendly"
                
        except Exception as e:
            self.logger.error(f"Tone determination failed: {e}")
            print(f"{YELLOW}âš ï¸ Tone determination error: {e}, defaulting to casual_friendly{RESET}")
            return "casual_friendly"  # Safe fallback
    
    def _fetch_user_description_from_server(self, session_id: str) -> str:
        """
        Fetch visual context from the Vision Context API.
        
        Args:
            session_id: Session ID to get visual context for
        
        Returns:
            str: Visual context description from the vision API, or empty string on failure
        """
        try:
            print(f"{BLUE}ðŸ“¸ Fetching visual context for session {session_id} from vision server...{RESET}")
            response = requests.get(
                f"{self.user_description_server_url}/visual-context/{session_id}",
                timeout=5
            )
            response.raise_for_status()
            
            data = response.json()
            
            # Check if visual context is available according to Vision API spec
            if data.get('available', False):
                visual_context = data.get('visual_context', '')
                print(f"{GREEN}âœ… Fetched visual context: '{visual_context}'{RESET}")
                return visual_context
            else:
                print(f"{YELLOW}âš ï¸ No visual context available for session {session_id}{RESET}")
                print(f"{YELLOW}   User may not have enabled vision or no recent analysis available{RESET}")
                return ""
            
        except requests.exceptions.ConnectionError:
            print(f"{YELLOW}âš ï¸ Could not connect to Vision Context API at {self.user_description_server_url}{RESET}")
            print(f"{YELLOW}   Make sure the vision API server is running on port 5004{RESET}")
            print(f"{YELLOW}   Using empty description (will default to casual_friendly tone){RESET}")
            return ""
        except requests.exceptions.Timeout:
            print(f"{YELLOW}âš ï¸ Vision Context API timeout{RESET}")
            return ""
        except Exception as e:
            self.logger.error(f"Failed to fetch visual context: {e}")
            print(f"{YELLOW}âš ï¸ Error fetching visual context: {e}{RESET}")
            return ""
    
    def _convert_tone(self, text: str, tone: str = "child_friendly", user_description: str = "", user_msg: str = "", is_first_message: bool = False) -> str:
        """
        Convert the tone/style of a given text using a secondary agent (LLM rewriter).
        
        Args:
            text: The original text to be rewritten.
            tone: Target tone/style (e.g., "child_friendly", "elder_friendly").
            user_description: Visual description from VLM (for additional context).
            user_msg: Original user message (for additional context).
        
        Returns:
            str | None: The rewritten text if successful, otherwise None.
        """
        try:
            if not text:
                return text

            # Detect input language
            has_chinese = any('\u4e00' <= char <= '\u9fff' for char in text)
            target_lang = "Traditional Chinese (ç¹é«”ä¸­æ–‡)" if has_chinese else "English"
            
            # Get appropriate system prompt based on tone
            system_prompt = get_tone_system_prompt(tone, target_lang)

            # Create tone-specific user instruction
            tone_descriptions = {
                "child_friendly": "children",
                "elder_friendly": "elderly people"
            }
            target_audience = tone_descriptions.get(tone, tone.replace("_", " "))
            
            # Enhanced instruction with user context and first message guidance
            context_info = ""
            if user_description:
                context_info += f"\nUser Appearance: {user_description}"
            if user_msg:
                context_info += f"\nUser Question: {user_msg}"
            
            # Add first message guidance
            if is_first_message and user_description:
                context_info += f"\nFirst Message: YES (MUST reference user appearance to grab attention)"
            elif user_description:
                context_info += f"\nFirst Message: NO ({PERCENTAGE}% chance to reference appearance for variety)"
            
            user_instruction = (
                f"Rewrite this text to speak to {target_audience} in {target_lang}:{context_info}\n"
                f"---\n{text}\n---\n\n"
                f"CRITICAL OUTPUT REQUIREMENTS:\n"
                f"- Strickly follow the convert mechanism about how to convert the answer to the right way\n"
                f"- Output ONLY the rewritten text - NO explanations, notes, prefixes, or meta-commentary\n"
                f"- Do NOT add any text after the rewritten message ends\n"
                f"- Do NOT include any notes like '(Note: ...)', '(I referenced...)', or similar explanations\n"
                f"- Do NOT add any follow-up text, comments, or clarifications\n"
                f"- The output must END immediately after the rewritten message - NO additional text whatsoever\n"
                f"- Start directly with the converted message and stop immediately when it ends"
            )

            payload = {
                "model": f"{LLM_MODEL_NAME}",
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_instruction},
                ],
                "stream": False,
                "options": {"temperature": 0.3}
            }

            resp = requests.post("http://localhost:11435/api/chat", json=payload, timeout=None)
            resp.raise_for_status()
            data = resp.json()
            if isinstance(data, dict) and "message" in data and isinstance(data["message"], dict):
                return data["message"].get("content", "").strip()
            # Fallback common formats
            if isinstance(data, dict) and "response" in data:
                return str(data.get("response", "")).strip()
            return None
        except Exception as e:
            self.logger.error(f"Tone conversion failed: {e}")
            return None

    def _stream_convert_tone(self, text: str, tone: str = "child_friendly", user_description: str = "", user_msg: str = "", is_first_message: bool = False):
        """
        Stream the tone conversion so users can see the rewritten text as it is generated.
        
        Args:
            text: The text to convert
            tone: Target tone/style (e.g., "child_friendly", "elder_friendly")
            user_description: Visual description from VLM (for additional context)
            user_msg: Original user message (for additional context)
        
        Yields chunks as they arrive and returns the final converted string.
        """
        try:
            if not text:
                yield text
                return

            # Detect input language
            has_chinese = any('\u4e00' <= char <= '\u9fff' for char in text)
            target_lang = "Traditional Chinese (ç¹é«”ä¸­æ–‡)" if has_chinese else "English"

            # Get appropriate system prompt based on tone
            system_prompt = get_tone_system_prompt(tone, target_lang)

            # Create tone-specific user instruction
            tone_descriptions = {
                "child_friendly": "children",
                "elder_friendly": "elderly people",
                "professional_friendly": "professional adult",
                "casual_friendly": "casual and chill adult"
            }
            target_audience = tone_descriptions.get(tone, tone.replace("_", " "))
            
            # Enhanced instruction with user context and first message guidance
            context_info = ""
            if user_description:
                context_info += f"\n[ä½¿ç”¨è€…å¤–è²Œæè¿°]: {user_description}"
            if user_msg:
                context_info += f"\n[ä½¿ç”¨è€…çš„åŽŸå§‹å•é¡Œ]: {user_msg}"
            
            # ç¬¬ä¸€è¼ªå°è©±çš„ç‰¹åˆ¥å¼•å°Ž
            first_msg_guide = ""
            if is_first_message and user_description:
                first_msg_guide = "\né€™æ˜¯æˆ‘èˆ‡å®¢äººçš„åˆæ¬¡è¦‹é¢ï¼Œè«‹å‹™å¿…åœ¨é–‹å ´æ™‚è¦ªåˆ‡åœ°æåˆ°å°æ–¹çš„å¤–è²Œç‰¹å¾µï¼ˆå¦‚ï¼šç´…å¸½å­ã€æ…ˆç¥¥çš„ç¬‘å®¹ï¼‰ä¾†æ‹‰è¿‘è·é›¢ã€‚"
            elif user_description:
                # é€™è£¡çš„ PERCENTAGE è®Šæ•¸è«‹åœ¨æ‚¨çš„ç¨‹å¼ç¢¼ä¸Šä¸‹æ–‡ä¸­å®šç¾©
                first_msg_guide = f"\né€™ä¸æ˜¯åˆæ¬¡è¦‹é¢ï¼Œè«‹è‡ªç„¶åœ°å°è©±ã€‚æœ‰ {PERCENTAGE}% çš„æ©ŸçŽ‡å¯ä»¥å†æ¬¡æåˆ°å°æ–¹çš„å¤–è²Œï¼Œå¢žåŠ è¦ªåˆ‡æ„Ÿã€‚"

            # çµ„åˆæœ€çµ‚çš„ user_instruction
            user_instruction = (
                f"### å°Žè¦½ä»»å‹™è³‡è¨Š ###\n"
                f"ç›®æ¨™èªžè¨€ï¼š{target_lang}\n"
                f"å°Žè¦½å°è±¡ï¼š{target_audience}\n"
                f"{context_info}\n"
                f"{first_msg_guide}\n"
                f"\n"
                f"### å¾…è½‰æ›çš„äº‹å¯¦å…§å®¹ï¼ˆPart 1 ç”¢å‡ºï¼‰ ###\n"
                f"---\n{text}\n---\n\n"
                f"### è¼¸å‡ºè¦ç¯„ ###\n"
                f"1. è«‹ä¾ç…§ã€Œè³‡æ·±å°Žè¦½å“¡ã€çš„èº«ä»½ï¼Œå°‡ä¸Šè¿°ã€äº‹å¯¦å…§å®¹ã€‘ç·¨ç¹”æˆä¸€æ®µæº«æš–çš„æ•…äº‹ã€‚\n"
                f"2. åš´ç¦ä½¿ç”¨ä»»ä½•è¡¨æƒ…ç¬¦è™Ÿ (Emoji)ã€‚\n"
                f"3. åƒ…è¼¸å‡ºè½‰æ›å¾Œçš„å°è©±æ–‡å­—ï¼Œä¸å¯åŒ…å«ä»»ä½•å‚™è¨»ã€è§£é‡‹ã€æ¨™ç±¤ï¼ˆå¦‚ã€Œå°Žè¦½å“¡ï¼šã€ï¼‰æˆ–æç¤ºè©žã€‚\n"
                f"4. ç¢ºä¿æ–‡å­—é€šé †ã€æœ‰é•·è¼©ç·£ï¼Œä¸¦è‡ªç„¶åœ°å¸¶å‡ºäº‹å¯¦å…§å®¹ä¸­çš„é—œéµæ•¸æ“šã€‚\n"
                f"5. è¨Šæ¯çµæŸå¾Œè«‹ç«‹å³åœæ­¢ï¼Œä¸è¦æœ‰ä»»ä½•å¤šé¤˜çš„çµèªžã€‚"
            )

            # é€™æ˜¯æ‚¨åŽŸæœ¬çš„ payload çµæ§‹
            payload = {
                "model": f"{LLM_MODEL_NAME}",
                "messages": [
                    {"role": "system", "content": system_prompt}, # é€™è£¡å‚³å…¥ build_elder_friendly_system_prompt
                    {"role": "user", "content": user_instruction},
                ],
                "stream": True,
                "options": {"temperature": 0.5}
            }

            response = requests.post("http://localhost:11435/api/chat", json=payload, stream=True)
            response.raise_for_status()
            
            converted = ""
            for line in response.iter_lines(decode_unicode=True):
                if not line:
                    continue
                try:
                    chunk = json.loads(line)
                except Exception:
                    continue
                message = chunk.get("message", {})
                delta = message.get("content")
                if delta:
                    converted += delta
                    yield delta
                if chunk.get("done"):
                    print(f"{RED}Converted msg: {converted}{RESET}")
                    yield "END_FLAG"
                    break
                    
        except Exception as e:
            self.logger.error(f"Streaming tone conversion failed: {e}")
            yield f"ERROR: {str(e)}"
            yield "END_FLAG"

    def _generate_streaming_response_with_tone(self, text_user_msg: str, session_id: str, chat_history: List[Dict], user_description: str = None, convert_tone: bool = True):
        """Generate streaming RAG + LLM response with dynamic tone conversion using parallel processing"""
        try:
            # Check if user_description is provided by client (textual input)
            # Only query Vision server if user_description is None or empty string
            has_client_provided_description = user_description and user_description.strip()
            
            if has_client_provided_description:
                print(f"{BLUE}ðŸ“ Using client-provided textual user description: '{user_description}'{RESET}")
                print(f"{BLUE}ðŸš€ Starting QA response generation (skipping Vision server query)...{RESET}")
            else:
                print(f"{BLUE}ðŸš€ Starting parallel processing for Vision API query and QA response...{RESET}")
            
            # Storage for parallel results
            fetched_user_description = ""
            response_content = ""
            
            # Create a thread pool for parallel execution (only if we need to query Vision server)
            if has_client_provided_description:
                # Client provided description - no need to query Vision server
                # Just generate QA response
                fetched_user_description = user_description.strip()
                
                print(f"{BLUE}ðŸ“ Collecting QA response with client-provided description...{RESET}")
                print(f"{BLUE}ðŸ“ Input: text_user_msg='{text_user_msg[:50]}...', session_id='{session_id}', chat_history_size={len(chat_history)}{RESET}")
                
                # Generate QA response directly (no parallel processing needed)
                # Pass user_description to LLM so it can consider visual context during response generation
                try:
                    print(f"{BLUE}ðŸ”„ Calling _generate_streaming_response with vision description...{RESET}")
                    original_response_generator = self._generate_streaming_response(
                        text_user_msg, session_id, chat_history, user_description=fetched_user_description
                    )
                    print(f"{GREEN}âœ… Generator created, starting iteration...{RESET}")
                    chunk_count = 0
                    for chunk in original_response_generator:
                        chunk_count += 1
                        if chunk == "END_FLAG":
                            print(f"{GREEN}âœ… Received END_FLAG after {chunk_count} chunks{RESET}")
                            break
                        elif chunk.startswith("ERROR:"):
                            print(f"{RED}âŒ Error in QA response: {chunk}{RESET}")
                            yield chunk
                            yield "END_FLAG"
                            return
                        else:
                            response_content += chunk
                            if chunk_count <= 5 or chunk_count % 50 == 0:  # Log first 5 chunks and every 50th
                                print(f"{BLUE}ðŸ“¦ Chunk #{chunk_count}: {len(chunk)} chars (total: {len(response_content)} chars){RESET}")
                    
                    print(f"{GREEN}âœ… QA response collected! Total chunks: {chunk_count}, Total length: {len(response_content)} chars{RESET}")
                    print(f"{BLUE}ðŸ“ Using client-provided user description: '{fetched_user_description}'{RESET}")
                    print(f"{RED}ðŸ’¬ QA response preview: '{response_content}' (full length: {len(response_content)}){RESET}")
                    
                    if not response_content.strip():
                        print(f"{YELLOW}âš ï¸ WARNING: QA response is empty!{RESET}")
                        yield "ERROR: QA response is empty"
                        yield "END_FLAG"
                        return
                        
                except Exception as e:
                    error_msg = f"Error collecting QA response: {str(e)}"
                    print(f"{RED}âŒ {error_msg}{RESET}")
                    self.logger.error(error_msg, exc_info=True)
                    yield f"ERROR: {error_msg}"
                    yield "END_FLAG"
                    return
            else:
                # No client-provided description - query Vision server in parallel with QA
                with ThreadPoolExecutor(max_workers=2) as executor:
                    # Task 1: Fetch visual context from Vision API (with 2-second delay)
                    def _delayed_fetch_user_description():
                        """Wait 2 seconds before fetching visual context from Vision server."""
                        print(f"{YELLOW}â³ Delaying Vision server fetch by 2 seconds...{RESET}")
                        time.sleep(2)
                        print(f"{BLUE}ðŸ“¸ Fetching visual context from Vision server for session ID: '{session_id[20:] if len(session_id) > 20 else session_id}'{RESET}")
                        return self._fetch_user_description_from_server(
                            session_id[20:] if len(session_id) > 20 else session_id
                        )

                    future_description = executor.submit(_delayed_fetch_user_description)
                    
                    # Task 2: Generate QA response
                    # Try to wait for vision description before starting QA (with timeout)
                    def collect_qa_response():
                        """Helper function to collect full QA response, waiting for vision description if available"""
                        try:
                            print(f"{BLUE}[Thread] Starting QA response collection...{RESET}")
                            print(f"[Thread] Input: text_user_msg='{text_user_msg[:50]}...', session_id='{session_id}', chat_history_size={len(chat_history)}{RESET}")
                            
                            # Try to get vision description first (wait up to 3 seconds)
                            # This allows LLM to see vision context during response generation
                            vision_desc = ""
                            try:
                                print(f"{BLUE}[Thread] Waiting for vision description (max 3 seconds)...{RESET}")
                                vision_desc = future_description.result(timeout=3)
                                print(f"{GREEN}[Thread] âœ… Got vision description: '{vision_desc}'{RESET}")
                            except Exception as e:
                                print(f"{YELLOW}[Thread] âš ï¸ Vision description not available yet or timeout: {e}{RESET}")
                                print(f"{YELLOW}[Thread] Proceeding without vision description in LLM context{RESET}")
                                vision_desc = ""
                            
                            content = ""
                            print(f"{BLUE}[Thread] Calling _generate_streaming_response with vision description...{RESET}")
                            original_response_generator = self._generate_streaming_response(
                                text_user_msg, session_id, chat_history, user_description=vision_desc
                            )
                            print(f"{GREEN}[Thread] Generator created, starting iteration...{RESET}")
                            
                            chunk_count = 0
                            for chunk in original_response_generator:
                                chunk_count += 1
                                if chunk == "END_FLAG":
                                    print(f"{GREEN}[Thread] âœ… Received END_FLAG after {chunk_count} chunks{RESET}")
                                    break
                                elif chunk.startswith("ERROR:"):
                                    print(f"{RED}[Thread] âŒ Error in QA response: {chunk}{RESET}")
                                    return None, chunk, vision_desc  # Return error and vision_desc
                                else:
                                    content += chunk
                                    if chunk_count <= 5 or chunk_count % 50 == 0:
                                        print(f"{BLUE}[Thread] ðŸ“¦ Chunk #{chunk_count}: {len(chunk)} chars (total: {len(content)} chars){RESET}")
                            
                            print(f"{GREEN}[Thread] âœ… QA response collected! Total chunks: {chunk_count}, Total length: {len(content)} chars{RESET}")
                            if not content.strip():
                                print(f"{YELLOW}[Thread] âš ï¸ WARNING: QA response is empty!{RESET}")
                            return content, None, vision_desc  # Return vision_desc so main thread can use it
                        except Exception as e:
                            error_msg = f"[Thread] Error in collect_qa_response: {str(e)}"
                            print(f"{RED}âŒ {error_msg}{RESET}")
                            import traceback
                            print(f"{RED}[Thread] Traceback: {traceback.format_exc()}{RESET}")
                            return None, f"ERROR: {error_msg}", ""
                    
                    future_qa = executor.submit(collect_qa_response)
                    
                    # Wait for both tasks to complete
                    print(f"{YELLOW}â³ Waiting for parallel tasks to complete...{RESET}")
                    
                    # Get QA response result (with timeout) - this also returns the vision description
                    try:
                        print(f"{YELLOW}â³ Waiting for QA response (this may take a while)...{RESET}")
                        result = future_qa.result(timeout=120)  # 2 minute timeout for LLM
                        response_content, error, fetched_user_description = result
                        
                        if error:
                            print(f"{RED}âŒ QA response collection failed: {error}{RESET}")
                            # Pass through errors immediately
                            yield error
                            yield "END_FLAG"
                            return
                        
                        # If vision description was not retrieved in collect_qa_response, try to get it now
                        if not fetched_user_description:
                            try:
                                print(f"{YELLOW}â³ Vision description not retrieved in QA thread, trying to get it now...{RESET}")
                                fetched_user_description = future_description.result(timeout=5)
                                print(f"{BLUE}ðŸ“¸ Vision server returned: '{fetched_user_description}'{RESET}")
                            except Exception as e:
                                print(f"{YELLOW}âš ï¸ Could not get vision description: {e}{RESET}")
                                fetched_user_description = ""
                        
                        print(f"{GREEN}âœ… Parallel tasks completed!{RESET}")
                        print(f"{BLUE}ðŸ“¸ User description from Vision server: '{fetched_user_description}'{RESET}")
                        print(f"{BLUE}ðŸ’¬ QA response length: {len(response_content)} chars{RESET}")
                        print(f"{BLUE}ðŸ’¬ QA response preview: '{response_content[:100]}...'{RESET}")
                        
                        if not response_content.strip():
                            print(f"{RED}âŒ ERROR: QA response is empty!{RESET}")
                            yield "ERROR: QA response is empty"
                            yield "END_FLAG"
                            return
                            
                    except Exception as e:
                        print(f"{RED}âŒ Error getting QA response result: {e}{RESET}")
                        import traceback
                        print(f"{RED}Traceback: {traceback.format_exc()}{RESET}")
                        yield f"ERROR: QA response collection failed: {str(e)}"
                        yield "END_FLAG"
                        return
            
            # Now apply tone conversion if requested
            print(f"{YELLOW}ðŸ” Checking tone conversion: convert_tone={convert_tone}, response_content length={len(response_content)}, response_content empty={not response_content.strip()}, has_description={bool(fetched_user_description and fetched_user_description.strip())}{RESET}")
            
            if not response_content.strip():
                print(f"{RED}âŒ ERROR: response_content is empty! Cannot proceed with tone conversion.{RESET}")
                yield "ERROR: QA response is empty"
                yield "END_FLAG"
                return
            
            if convert_tone and response_content.strip():
                # Determine tone based on user description
                if fetched_user_description and fetched_user_description.strip():
                    print(f"{BLUE}ðŸŽ¯ Determining tone from user description: '{fetched_user_description}'{RESET}")
                    selected_tone = self._determine_tone_from_user_description(fetched_user_description)
                    description_source = "client-provided text" if has_client_provided_description else "Vision server"
                    print(f"{BLUE}ðŸŽ¨ Converting tone to {selected_tone} (determined from {description_source}: '{fetched_user_description}')...{RESET}")
                else:
                    selected_tone = "casual_friendly"  # Default fallback
                    print(f"{BLUE}ðŸŽ¨ Converting tone to {selected_tone} (default, no user description available)...{RESET}")
                
                # Check if this is the first message in the conversation
                is_first_message = len(chat_history) == 0
                print(f"{YELLOW}chat history: {chat_history}{RESET}")
                print(f"{BLUE}ðŸŽ¯ Is first message: {is_first_message} (chat history size: {int(len(chat_history) / 2)}){RESET}")
                
                # Stream the tone conversion with user context
                print(f"{BLUE}ðŸ”„ Starting tone conversion stream...{RESET}")
                converted_response = ""
                tone_chunk_count = 0
                for tone_chunk in self._stream_convert_tone(
                    response_content, 
                    selected_tone, 
                    user_description=fetched_user_description,
                    user_msg=text_user_msg,
                    is_first_message=is_first_message
                ):
                    tone_chunk_count += 1
                    if tone_chunk == "END_FLAG":
                        # # Store the CONVERTED response to chat history (not original)
                        # Store the ORIGINAL (pre-tone-convert) response in chat history
                        if session_id not in self.chat_sessions:
                            self.chat_sessions[session_id] = []
                        
                        self.chat_sessions[session_id].append({"role": "user", "content": text_user_msg})
                        # self.chat_sessions[session_id].append({"role": "assistant", "content": converted_response})
                        self.chat_sessions[session_id].append({"role": "assistant", "content": response_content})
                        print(f"{GREEN}ðŸŽ¨ Tone conversion completed! Total chunks: {tone_chunk_count}, Converted length: {len(converted_response)} chars{RESET}")
                        # print(f"{GREEN}ðŸŽ¨ Chat history updated with tone-converted response for session {session_id}{RESET}")
                        print(f"{GREEN}ðŸŽ¨ Chat history updated with ORIGINAL (pre-tone) response for session {session_id}{RESET}")
                        
                        yield "END_FLAG"
                        break
                    else:
                        converted_response += tone_chunk
                        if tone_chunk_count <= 5 or tone_chunk_count % 50 == 0:
                            print(f"{BLUE}ðŸŽ¨ Tone chunk #{tone_chunk_count}: {len(tone_chunk)} chars (total: {len(converted_response)} chars){RESET}")
                        yield tone_chunk
            else:
                # Just return the original response if no tone conversion
                # Add both user message and original response to chat history
                if session_id not in self.chat_sessions:
                    self.chat_sessions[session_id] = []
                
                self.chat_sessions[session_id].append({"role": "user", "content": text_user_msg})
                self.chat_sessions[session_id].append({"role": "assistant", "content": response_content})
                print(f"{GREEN}ðŸ¤– Chat history updated with original response for session {session_id}{RESET}")
                yield response_content
                yield "END_FLAG"
                
        except Exception as e:
            self.logger.error(f"Streaming response with tone error: {e}")
            yield f"ERROR: {str(e)}"
            yield "END_FLAG"

    def _generate_streaming_response(self, text_user_msg: str, session_id: str, chat_history: List[Dict], user_description: str = None) -> str:
        """Generate streaming RAG + LLM response with optional vision description context"""
        try:
            context = ""
            
            # Step 1: Rewrite query for better retrieval (especially for follow-up questions)
            rewritten_query = text_user_msg
            if self.rag_initialized and self.rag_pipeline and self.chroma_collection:
                if chat_history:  # Only rewrite if there's conversation history (follow-up questions)
                    print(f"{BLUE}ðŸ”„ Step 1: Rewriting query for better retrieval...{RESET}")
                    rewritten_query = self._rewrite_query(text_user_msg, chat_history)
                else:
                    print(f"{BLUE}ðŸ“ Using original query (no history to rewrite){RESET}")
            
            # Step 2: Get RAG context using rewritten query
            if self.rag_initialized and self.rag_pipeline and self.chroma_collection:
                try:
                    print(f"{BLUE}ðŸ” Step 2: Searching with rewritten query: '{rewritten_query}'{RESET}")
                    search_results = self.rag_pipeline.hybrid_search(
                        rewritten_query, self.chroma_collection, top_k=6
                    )
                    
                    # Extract museum context
                    museum_context = []
                    for result in search_results:
                        content = result.get('content', '')
                        if content and not (content.startswith('[Q') or content.startswith('[A')):
                            museum_context.append(content)
                    
                    # Step 3: Process context and use original question for final response generation
                    context = self.rag_pipeline._process_context(museum_context, text_user_msg)
                    print(f"{BLUE}ðŸ“ Step 3: Using original question '{text_user_msg}' for response generation{RESET}")
                    print(f"{YELLOW}ðŸ“š RAG CONTEXT: {len(context)} chars{RESET}")
                    print(f"{GRAY}RAG DATA: {context}{RESET}")
                    
                except Exception as e:
                    print(f"{YELLOW}âš ï¸ RAG search failed: {e}{RESET}")
                    context = ""
            else:
                print(f"{YELLOW}âš ï¸ RAG not available, using LLM-only mode{RESET}")
                context = ""
            
            # Build messages for LLM
            if self.rag_initialized and self.rag_pipeline:
                # Use RAG pipeline's message building
                system_prompt = self.rag_pipeline.cached_system_prompt
                # print(f"{YELLOW}ðŸ¤– SYSTEM PROMPT: {system_prompt}{RESET}")
                # Pass rewritten_query to help QA agent understand user intent better
                messages = self.rag_pipeline._build_messages(
                    system_prompt, text_user_msg, chat_history, context, user_description, rewritten_query
                )
                if user_description and user_description.strip():
                    print(f"{BLUE}ðŸ‘ï¸ Vision description included in LLM context: '{user_description[:50]}...'{RESET}")
                print(f"{YELLOW}ðŸ¤– chat_history size: {int(len(chat_history) / 2)}{RESET}")
            else:
                # Fallback to simple messages
                messages = [
                    {"role": "system", "content": "You are a helpful AI assistant. Keep your responses concise and engaging."},
                    {"role": "user", "content": text_user_msg}
                ]
            
            # Stream LLM response
            request_payload = {
                "model": f"{LLM_MODEL_NAME}",
                "messages": messages,
                "stream": True,
                "options": {"temperature": 0.7}
            }
            
            print(f"{YELLOW}ðŸ¤– Starting streaming LLM response for session {session_id}...{RESET}")
            response = requests.post("http://localhost:11435/api/chat", json=request_payload, stream=True)
            response.raise_for_status()
            
            # Process streaming response
            response_content = ""
            for line in response.iter_lines(decode_unicode=True):
                if not line:
                    continue
                
                try:
                    chunk = json.loads(line)
                except Exception:
                    continue
                
                message = chunk.get("message", {})
                delta = message.get("content")
                
                if delta:
                    response_content += delta
                    # Stream the delta to client
                    yield delta
                
                if chunk.get("done"):
                    # Note: Chat history is now updated in the calling method after tone conversion
                    # Keep only recent history (last 10 messages)
                    # if len(self.chat_sessions[session_id]) > 10:
                    #     self.chat_sessions[session_id] = self.chat_sessions[session_id][-10:]
                    
                    print(f"{GREEN}ðŸ¤– Streaming response completed for session {session_id}: {len(response_content)} chars{RESET}")
                    # print(f"{YELLOW}ðŸ¤– Response content (before tone conversion): {response_content}{RESET}")
                    
                    # Send END_FLAG when done
                    yield "END_FLAG"
                    break
        except Exception as e:
            self.logger.error(f"Streaming response error: {e}")
            yield f"ERROR: {str(e)}"
            yield "END_FLAG"
    
    def _warmup_embedding_model(self) -> Dict[str, Any]:
        """Warmup the embedding model by performing a small embedding operation"""
        start_time = time.time()
        
        try:
            if not self.rag_initialized or not self.chroma_collection:
                return {
                    'status': 'skipped',
                    'message': 'RAG system not initialized',
                    'time_ms': 0
                }
            
            print(f"{BLUE}ðŸ”¥ Warming up embedding model...{RESET}")
            
            # Perform a small query to warm up the embedding model
            warmup_query = "ITRI warmup test"
            
            # Prefer using collection's own embedding function to avoid dimension mismatches
            try:
                result = self.chroma_collection.query(
                    query_texts=[warmup_query],
                    n_results=1
                )
            except Exception as e:
                # Fallback to manual embedding if collection has no embedding function configured
                print(f"{YELLOW}âš ï¸ query_texts warmup failed, falling back to manual embedding: {e}{RESET}")
                q_response = requests.post("http://localhost:11435/api/embeddings", json={
                    "model": "bge-m3:latest",
                    "prompt": warmup_query
                })
                q_response.raise_for_status()
                q_emb = [q_response.json()['embedding']]
                
                result = self.chroma_collection.query(
                    query_embeddings=q_emb,
                    n_results=1
                )
            
            elapsed_ms = (time.time() - start_time) * 1000
            
            print(f"{GREEN}âœ… Embedding model warmed up in {elapsed_ms:.0f}ms{RESET}")
            
            return {
                'status': 'success',
                'message': f'Embedding model warmed up successfully',
                'time_ms': round(elapsed_ms, 2),
                'test_query': warmup_query,
                'results_found': len(result.get('documents', []))
            }
            
        except Exception as e:
            elapsed_ms = (time.time() - start_time) * 1000
            error_str = str(e)
            
            # Handle specific embedding dimension mismatch
            if "expecting embedding with dimension" in error_str:
                print(f"{YELLOW}âš ï¸ Embedding dimension mismatch detected{RESET}")
                print(f"{YELLOW}   This usually means the collection was created with a different embedding model{RESET}")
                print(f"{YELLOW}   Embedding warmup will be skipped, but this won't affect LLM performance{RESET}")
                
                return {
                    'status': 'skipped',
                    'message': f'Embedding dimension mismatch - collection and current model incompatible',
                    'time_ms': round(elapsed_ms, 2),
                    'details': error_str,
                    'recommendation': 'Consider recreating the collection with the current embedding model'
                }
            else:
                # Other embedding errors
                error_msg = f"Embedding warmup failed: {error_str}"
                print(f"{YELLOW}âš ï¸ {error_msg}{RESET}")
                
                return {
                    'status': 'failed',
                    'message': error_msg,
                    'time_ms': round(elapsed_ms, 2)
                }
    
    def _warmup_llm_model(self) -> Dict[str, Any]:
        """Warmup the LLM by sending a small test request"""
        start_time = time.time()
        
        try:
            print(f"{BLUE}ðŸ”¥ Warming up LLM model...{RESET}")
            
            # Send a minimal request to warm up the LLM
            warmup_messages = [
                {"role": "system", "content": "You are a helpful assistant. Respond with just 'OK'."},
                {"role": "user", "content": "Warmup test"}
            ]
            
            request_payload = {
                "model": f"{LLM_MODEL_NAME}",
                "messages": warmup_messages,
                "stream": False,  # Non-streaming for faster warmup
                "options": {
                    "temperature": 0.1,
                    # "max_tokens": 5  # Very short response
                }
            }
            
            response = requests.post(
                "http://localhost:11435/api/chat", 
                json=request_payload, 
                timeout=None  # 30 second timeout for warmup
            )
            response.raise_for_status()
            
            result = response.json()
            elapsed_ms = (time.time() - start_time) * 1000
            
            # Check if we got a valid response
            if 'message' in result and 'content' in result['message']:
                response_content = result['message']['content']
                print(f"{GREEN}âœ… LLM model warmed up in {elapsed_ms:.0f}ms{RESET}")
                
                return {
                    'status': 'success',
                    'message': 'LLM model warmed up successfully',
                    'time_ms': round(elapsed_ms, 2),
                    'test_response': response_content.strip()
                }
            else:
                return {
                    'status': 'failed',
                    'message': 'LLM returned unexpected response format',
                    'time_ms': round(elapsed_ms, 2)
                }
            
        except requests.exceptions.Timeout:
            elapsed_ms = (time.time() - start_time) * 1000
            error_msg = "LLM warmup timed out (30s)"
            print(f"{YELLOW}âš ï¸ {error_msg}{RESET}")
            
            return {
                'status': 'failed',
                'message': error_msg,
                'time_ms': round(elapsed_ms, 2)
            }
            
        except requests.exceptions.ConnectionError:
            elapsed_ms = (time.time() - start_time) * 1000
            error_msg = "Could not connect to LLM service (check if Ollama is running)"
            print(f"{YELLOW}âš ï¸ {error_msg}{RESET}")
            
            return {
                'status': 'failed',
                'message': error_msg,
                'time_ms': round(elapsed_ms, 2)
            }
            
        except Exception as e:
            elapsed_ms = (time.time() - start_time) * 1000
            error_msg = f"LLM warmup failed: {str(e)}"
            print(f"{YELLOW}âš ï¸ {error_msg}{RESET}")
            
            return {
                'status': 'failed',
                'message': error_msg,
                'time_ms': round(elapsed_ms, 2)
            }
    
    def run(self, host: str = '0.0.0.0', port: int = 5002, debug: bool = False):
        """Run the API service"""
        print(f"{GREEN}ðŸš€ RAG + LLM API Service Starting{RESET}")
        print("=" * 70)
        print(f"ðŸŒ Service URL: http://{host}:{port}")
        print(f"ðŸ“‹ Health Check: GET http://{host}:{port}/health")
        print(f"ðŸ¤– Query Endpoint: POST http://{host}:{port}/api/rag-llm/query")
        print(f"ðŸŽ¨ Tone Convert: POST http://{host}:{port}/api/rag-llm/convert-tone")
        print(f"ðŸ¤–ðŸŽ¨ Query + Dynamic Tone: POST http://{host}:{port}/api/rag-llm/query-with-tone")
        print(f"ðŸ”„ Init Endpoint: POST http://{host}:{port}/api/rag-llm/init")
        print(f"ðŸ”¥ Warmup Endpoint: POST http://{host}:{port}/api/rag-llm/warmup")
        print(f"ðŸ‘‹ Close Endpoint: POST http://{host}:{port}/api/rag-llm/close")
        print("=" * 70)
        
        self.app.run(host=host, port=port, debug=debug, threaded=True)

def main():
    """Main entry point"""
    import argparse
    
    parser = argparse.ArgumentParser(description='RAG + LLM API Service')
    parser.add_argument('--host', default='0.0.0.0', help='Host to bind to')
    parser.add_argument('--port', type=int, default=5002, help='Port to bind to')
    parser.add_argument('--debug', action='store_true', help='Enable debug mode')
    parser.add_argument('--auto-init', action='store_true', help='Auto-initialize RAG system on startup')
    parser.add_argument('--user-description-server', default='http://localhost:5004', 
                        help='URL of the Vision Context API server (default: http://localhost:5004)')
    
    args = parser.parse_args()
    
    # Create and configure service
    service = RAGLLMAPIService(user_description_server_url=args.user_description_server)
    
    # Auto-initialize if requested
    if args.auto_init:
        print(f"{BLUE}ðŸ”„ Auto-initializing RAG system...{RESET}")
        if service._initialize_rag_system():
            print(f"{GREEN}âœ… RAG system initialized{RESET}")
        else:
            print(f"{RED}âŒ RAG initialization failed{RESET}")
            return 1
    
    # Run the service
    try:
        service.run(host=args.host, port=args.port, debug=args.debug)
    except KeyboardInterrupt:
        print(f"\n{BLUE}ðŸ‘‹ API service shutting down...{RESET}")
        return 0
    except Exception as e:
        print(f"\n{RED}ðŸ’¥ Service error: {e}{RESET}")
        return 1

if __name__ == "__main__":
    exit(main())