# ITRI Scrapy Crawler ğŸ•·ï¸

åŸºæ–¼ Scrapy æ¡†æ¶çš„å·¥æ¥­æŠ€è¡“ç ”ç©¶é™¢ (ITRI) ç¶²è·¯è³‡æ–™çˆ¬èŸ²ç³»çµ±ï¼Œå°ˆç‚ºæ”¶é›† ITRI ç›¸é—œçš„æŠ€è¡“è³‡è¨Šã€æ–°èå ±å°å’Œç ”ç©¶å…§å®¹è€Œè¨­è¨ˆã€‚

## ğŸ¯ å°ˆæ¡ˆç‰¹è‰²

- **å¤šæºçˆ¬å–**: æ”¯æ´ ITRI å®˜ç¶²ã€Wikipediaã€æ–°èåª’é«”ç­‰å¤šå€‹è³‡æ–™æº
- **æ™ºæ…§éæ¿¾**: è‡ªå‹•è­˜åˆ¥å’Œéæ¿¾ ITRI ç›¸é—œå…§å®¹
- **è³‡æ–™æ¸…ç†**: å…§å»ºè³‡æ–™é©—è­‰å’Œæ¸…ç†ç®¡é“
- **å»é‡æ©Ÿåˆ¶**: è‡ªå‹•éæ¿¾é‡è¤‡å…§å®¹
- **å“è³ªè©•åˆ†**: ç‚ºæ¯å€‹å…§å®¹é …ç›®è¨ˆç®—å“è³ªåˆ†æ•¸
- **çµæ§‹åŒ–è¼¸å‡º**: ç”Ÿæˆ JSON æ ¼å¼çš„çµæ§‹åŒ–è³‡æ–™
- **è©³ç´°å ±å‘Š**: æä¾›å®Œæ•´çš„çˆ¬å–çµ±è¨ˆå’Œå“è³ªå ±å‘Š

## ğŸ“ å°ˆæ¡ˆçµæ§‹

```
dataset_202412_classic/
â”œâ”€â”€ itri_scrapy_crawler/           # Scrapy å°ˆæ¡ˆç›®éŒ„
â”‚   â”œâ”€â”€ itri_scrapy_crawler/
â”‚   â”‚   â”œâ”€â”€ spiders/               # çˆ¬èŸ²ç¨‹å¼
â”‚   â”‚   â”‚   â”œâ”€â”€ itri_official.py   # ITRI å®˜ç¶²çˆ¬èŸ²
â”‚   â”‚   â”‚   â”œâ”€â”€ itri_wikipedia.py  # Wikipedia çˆ¬èŸ²
â”‚   â”‚   â”‚   â””â”€â”€ itri_news.py       # æ–°èåª’é«”çˆ¬èŸ²
â”‚   â”‚   â”œâ”€â”€ items.py               # è³‡æ–™é …ç›®å®šç¾©
â”‚   â”‚   â”œâ”€â”€ pipelines.py           # è³‡æ–™è™•ç†ç®¡é“
â”‚   â”‚   â”œâ”€â”€ settings.py            # çˆ¬èŸ²è¨­å®š
â”‚   â”‚   â””â”€â”€ middlewares.py         # ä¸­ä»‹è»Ÿé«”
â”‚   â””â”€â”€ scrapy.cfg                 # Scrapy è¨­å®šæª”
â”œâ”€â”€ run_itri_crawler.py            # ä¸»è¦åŸ·è¡Œè…³æœ¬
â”œâ”€â”€ requirements.txt               # ä¾è³´å¥—ä»¶
â”œâ”€â”€ README.md                      # èªªæ˜æ–‡ä»¶
â””â”€â”€ crawled_data/                  # è¼¸å‡ºè³‡æ–™ç›®éŒ„
```

## ğŸš€ å¿«é€Ÿé–‹å§‹

### 1. å®‰è£ä¾è³´

```bash
cd dataset_202412_classic
pip install -r requirements.txt
```

### 2. æª¢æŸ¥è¨­å®š

```bash
python run_itri_crawler.py --check
```

### 3. æŸ¥çœ‹å¯ç”¨çˆ¬èŸ²

```bash
python run_itri_crawler.py --list
```

### 4. åŸ·è¡Œæ‰€æœ‰çˆ¬èŸ²

```bash
python run_itri_crawler.py
```

### 5. åŸ·è¡Œç‰¹å®šçˆ¬èŸ²

```bash
# åªçˆ¬å– ITRI å®˜ç¶²
python run_itri_crawler.py --spiders itri_official

# çˆ¬å–å®˜ç¶²å’Œ Wikipedia
python run_itri_crawler.py --spiders itri_official itri_wikipedia
```

## ğŸ•·ï¸ çˆ¬èŸ²èªªæ˜

### 1. ITRI å®˜ç¶²çˆ¬èŸ² (`itri_official`)
- **ç›®æ¨™**: https://www.itri.org.tw
- **å…§å®¹**: å®˜æ–¹æ–°èã€ç ”ç©¶æˆæœã€æŠ€è¡“æœå‹™ã€ç”¢æ¥­åˆä½œè³‡è¨Š
- **é ä¼°æ™‚é–“**: 10-15 åˆ†é˜
- **ç‰¹è‰²**: 
  - æ™ºæ…§å…§å®¹åˆ†é¡ (æ–°è/ç ”ç©¶/æœå‹™)
  - è‡ªå‹•æå–éƒ¨é–€å’Œè¯çµ¡è³‡è¨Š
  - æ”¯æ´ä¸­è‹±æ–‡å…§å®¹

### 2. Wikipedia çˆ¬èŸ² (`itri_wikipedia`)
- **ç›®æ¨™**: Wikipedia (ä¸­æ–‡/è‹±æ–‡)
- **å…§å®¹**: ITRI ç›¸é—œç™¾ç§‘æ¢ç›®ã€æŠ€è¡“è©æ¢ã€ç›¸é—œæ©Ÿæ§‹è³‡è¨Š
- **é ä¼°æ™‚é–“**: 5-10 åˆ†é˜
- **ç‰¹è‰²**:
  - ä½¿ç”¨ Wikipedia API æœå°‹
  - è‡ªå‹•è¿½è¹¤ç›¸é—œé€£çµ
  - éæ¿¾éç›¸é—œå…§å®¹

### 3. æ–°èåª’é«”çˆ¬èŸ² (`itri_news`)
- **ç›®æ¨™**: Google Newsã€ç§‘æŠ€åª’é«”ç¶²ç«™
- **å…§å®¹**: ITRI ç›¸é—œæ–°èå ±å°ã€åª’é«”å ±å°ã€ç”¢æ¥­å‹•æ…‹
- **é ä¼°æ™‚é–“**: 15-20 åˆ†é˜
- **ç‰¹è‰²**:
  - å¤šåª’é«”ä¾†æºæ•´åˆ
  - æ–°èé¡å‹è‡ªå‹•åˆ†é¡
  - æ™‚æ•ˆæ€§å…§å®¹å„ªå…ˆ

## ğŸ“Š è³‡æ–™è¼¸å‡º

### è¼¸å‡ºæ ¼å¼
æ‰€æœ‰çˆ¬å–çš„è³‡æ–™éƒ½æœƒä»¥ JSON æ ¼å¼å„²å­˜ï¼Œæ¯å€‹é …ç›®åŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š

```json
{
  "id": "å”¯ä¸€è­˜åˆ¥ç¢¼",
  "title": "æ¨™é¡Œ",
  "content": "æ¸…ç†å¾Œçš„å…§å®¹",
  "url": "åŸå§‹ç¶²å€",
  "source": "è³‡æ–™ä¾†æº",
  "language": "èªè¨€ (zh-tw/en)",
  "content_type": "å…§å®¹é¡å‹",
  "crawled_at": "çˆ¬å–æ™‚é–“",
  "category": "åˆ†é¡",
  "tags": ["æ¨™ç±¤åˆ—è¡¨"],
  "summary": "æ‘˜è¦",
  "quality_score": "å“è³ªåˆ†æ•¸ (0-1)",
  "metadata": {
    "research_area": "ç ”ç©¶é ˜åŸŸ",
    "technology_type": "æŠ€è¡“é¡å‹",
    "keywords": ["é—œéµå­—"]
  }
}
```

### è¼¸å‡ºæª”æ¡ˆ
```
crawled_data/
â”œâ”€â”€ crawl_20241218_143052/         # çˆ¬å–æœƒè©±ç›®éŒ„
â”‚   â”œâ”€â”€ itri_official_articles.json    # ITRI å®˜ç¶²å…§å®¹
â”‚   â”œâ”€â”€ itri_wikipedia_articles.json   # Wikipedia å…§å®¹
â”‚   â”œâ”€â”€ itri_news_articles.json        # æ–°èå…§å®¹
â”‚   â”œâ”€â”€ all_articles_combined.json     # æ‰€æœ‰å…§å®¹åˆä½µ
â”‚   â””â”€â”€ crawl_statistics.json          # è©³ç´°çµ±è¨ˆ
â”œâ”€â”€ session_report.md              # æœƒè©±å ±å‘Š
â”œâ”€â”€ session_report.json            # æœƒè©±å ±å‘Š (JSON)
â””â”€â”€ *.log                          # çˆ¬èŸ²æ—¥èªŒ
```

## âš™ï¸ è¨­å®šé¸é …

### çˆ¬èŸ²è¨­å®š (`settings.py`)
- **DOWNLOAD_DELAY**: è«‹æ±‚é–“éš” (é è¨­: 2 ç§’)
- **CONCURRENT_REQUESTS**: ä¸¦ç™¼è«‹æ±‚æ•¸ (é è¨­: 16)
- **USER_AGENT**: ä½¿ç”¨è€…ä»£ç†å­—ä¸²
- **ROBOTSTXT_OBEY**: éµå®ˆ robots.txt (é è¨­: True)

### è‡ªè¨‚è¨­å®š
```python
# åœ¨ settings.py ä¸­ä¿®æ”¹
ITRI_CRAWLER_SETTINGS = {
    "MAX_PAGES_PER_SPIDER": 100,      # æ¯å€‹çˆ¬èŸ²æœ€å¤§é é¢æ•¸
    "MIN_CONTENT_LENGTH": 50,         # æœ€å°å…§å®¹é•·åº¦
    "PREFERRED_LANGUAGES": ["zh-tw", "en"],  # åå¥½èªè¨€
    "OUTPUT_DIR": "crawled_data",     # è¼¸å‡ºç›®éŒ„
    "ENHANCE_CONTENT": True,          # å•Ÿç”¨å…§å®¹å¢å¼·
}
```

## ğŸ”§ é€²éšä½¿ç”¨

### 1. å–®ç¨åŸ·è¡Œçˆ¬èŸ²
```bash
cd itri_scrapy_crawler
scrapy crawl itri_official
```

### 2. è‡ªè¨‚è¼¸å‡ºæ ¼å¼
```bash
scrapy crawl itri_official -o output.json -t json
```

### 3. èª¿è©¦æ¨¡å¼
```bash
scrapy crawl itri_official -L DEBUG
```

### 4. ä½¿ç”¨å¿«å– (é–‹ç™¼ç”¨)
åœ¨ `settings.py` ä¸­è¨­å®š:
```python
HTTPCACHE_ENABLED = True
```

## ğŸ“ˆ è³‡æ–™å“è³ª

### å“è³ªæ§åˆ¶æ©Ÿåˆ¶
1. **å…§å®¹é©—è­‰**: æª¢æŸ¥å¿…è¦æ¬„ä½å’Œå…§å®¹é•·åº¦
2. **å»é‡éæ¿¾**: åŸºæ–¼å…§å®¹é›œæ¹Šå€¼å»é™¤é‡è¤‡
3. **ç›¸é—œæ€§æª¢æŸ¥**: ç¢ºä¿å…§å®¹èˆ‡ ITRI ç›¸é—œ
4. **å“è³ªè©•åˆ†**: åŸºæ–¼å¤šå€‹å› å­è¨ˆç®—å“è³ªåˆ†æ•¸
5. **è³‡æ–™æ¸…ç†**: ç§»é™¤å°èˆªã€å»£å‘Šç­‰é›œè¨Šå…§å®¹

### å“è³ªåˆ†æ•¸è¨ˆç®—
- **åŸºç¤åˆ†æ•¸**: 0.5
- **å…§å®¹é•·åº¦**: +0.1 (>200å­—) +0.1 (>500å­—) +0.1 (>1000å­—)
- **ITRI ç›¸é—œæ€§**: +0.1 (åŒ…å« ITRI é—œéµå­—)
- **æŠ€è¡“å…§å®¹**: +0.1 (åŒ…å«æŠ€è¡“è©å½™)
- **çµæ§‹å®Œæ•´æ€§**: +0.05 (æœ‰åˆ†é¡) +0.05 (æœ‰æ¨™ç±¤)

## ğŸ”— èˆ‡ RAG ç³»çµ±æ•´åˆ

### æ•´åˆç¯„ä¾‹
```python
import json
from your_rag_system import DocumentChunk, RAGPipeline

# è¼‰å…¥çˆ¬å–çš„è³‡æ–™
with open('crawled_data/crawl_latest/all_articles_combined.json', 'r') as f:
    itri_data = json.load(f)

# è½‰æ›ç‚º RAG ç³»çµ±æ ¼å¼
chunks = []
for item in itri_data:
    if item['quality_score'] >= 0.6:  # åªä½¿ç”¨é«˜å“è³ªå…§å®¹
        chunk = DocumentChunk(
            content=item['content'],
            chunk_id=item['id'],
            source_file=item['url'],
            metadata={
                'title': item['title'],
                'source': item['source'],
                'summary': item['summary'],
                'tags': item['tags'],
                'quality_score': item['quality_score']
            }
        )
        chunks.append(chunk)

# å»ºç«‹å‘é‡è³‡æ–™åº«
pipeline = RAGPipeline()
pipeline.build_vector_store(chunks, collection_name='itri_knowledge_2024')
```

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

1. **Scrapy æœªå®‰è£**
   ```bash
   pip install scrapy
   ```

2. **æ¬Šé™éŒ¯èª¤**
   - ç¢ºä¿æœ‰å¯«å…¥ `crawled_data` ç›®éŒ„çš„æ¬Šé™
   - é¿å…ä½¿ç”¨ sudo åŸ·è¡Œ

3. **ç¶²è·¯é€£ç·šå•é¡Œ**
   - æª¢æŸ¥ç¶²è·¯é€£ç·š
   - èª¿æ•´ `DOWNLOAD_DELAY` è¨­å®š

4. **è¨˜æ†¶é«”ä¸è¶³**
   - æ¸›å°‘ `CONCURRENT_REQUESTS`
   - èª¿æ•´ `MEMUSAGE_LIMIT_MB`

### æ—¥èªŒåˆ†æ
```bash
# æŸ¥çœ‹çˆ¬èŸ²æ—¥èªŒ
tail -f crawled_data/itri_official.log

# æœå°‹éŒ¯èª¤
grep ERROR crawled_data/*.log
```

## ğŸ“‹ é–‹ç™¼æŒ‡å—

### æ–°å¢çˆ¬èŸ²
1. åœ¨ `spiders/` ç›®éŒ„å»ºç«‹æ–°çš„ `.py` æª”æ¡ˆ
2. ç¹¼æ‰¿é©ç•¶çš„ Spider é¡åˆ¥
3. å®šç¾© `start_urls` å’Œ `parse` æ–¹æ³•
4. åœ¨ `run_itri_crawler.py` ä¸­è¨»å†Šæ–°çˆ¬èŸ²

### è‡ªè¨‚è³‡æ–™è™•ç†
1. åœ¨ `pipelines.py` ä¸­æ–°å¢è™•ç†ç®¡é“
2. åœ¨ `settings.py` ä¸­è¨»å†Šç®¡é“
3. è¨­å®šé©ç•¶çš„å„ªå…ˆé †åº

### æ¸¬è©¦
```bash
# åŸ·è¡Œæ¸¬è©¦
pytest tests/

# æ¸¬è©¦ç‰¹å®šçˆ¬èŸ²
scrapy check itri_official
```

## ğŸ“œ æˆæ¬Šæ¢æ¬¾

æœ¬å°ˆæ¡ˆéµå¾ª MIT æˆæ¬Šæ¢æ¬¾ã€‚è«‹ç¢ºä¿åœ¨ä½¿ç”¨çˆ¬å–çš„è³‡æ–™æ™‚éµå®ˆå„ç¶²ç«™çš„ä½¿ç”¨æ¢æ¬¾å’Œ robots.txt è¦ç¯„ã€‚

## ğŸ¤ è²¢ç»æŒ‡å—

æ­¡è¿æäº¤ Issue å’Œ Pull Request ä¾†æ”¹å–„é€™å€‹å°ˆæ¡ˆï¼

### é–‹ç™¼ç’°å¢ƒè¨­å®š
```bash
git clone <repository>
cd dataset_202412_classic
pip install -r requirements.txt
pip install -e .
```

### ç¨‹å¼ç¢¼é¢¨æ ¼
- ä½¿ç”¨ Black é€²è¡Œç¨‹å¼ç¢¼æ ¼å¼åŒ–
- ä½¿ç”¨ flake8 é€²è¡Œç¨‹å¼ç¢¼æª¢æŸ¥
- éµå¾ª PEP 8 è¦ç¯„

---

**ç”± ITRI Scrapy Crawler ç”Ÿæˆ - å°ˆç‚ºå·¥æ¥­æŠ€è¡“ç ”ç©¶é™¢è³‡æ–™æ”¶é›†è€Œè¨­è¨ˆ** ğŸ¤–












