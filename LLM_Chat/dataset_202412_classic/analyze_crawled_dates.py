#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ITRI Crawled Data Date Analysis
==============================
åˆ†æçˆ¬å–æ•¸æ“šçš„æ™‚é–“åˆ†å¸ƒï¼Œå¹«åŠ©è¨­å®šåˆé©çš„æ™‚é–“éæ¿¾æ¢ä»¶
"""

import json
import re
from datetime import datetime, timedelta
from pathlib import Path
from collections import defaultdict, Counter
import argparse

def analyze_dates_in_file(file_path):
    """åˆ†æå–®å€‹ JSON æ–‡ä»¶ä¸­çš„æ—¥æœŸåˆ†å¸ƒ"""
    print(f"ğŸ“Š åˆ†ææ–‡ä»¶: {file_path}")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"âŒ ç„¡æ³•è®€å–æ–‡ä»¶: {e}")
        return None
    
    if not isinstance(data, list):
        print(f"âš ï¸  æ–‡ä»¶æ ¼å¼ä¸æ­£ç¢º (éæ•¸çµ„)")
        return None
    
    # çµ±è¨ˆæ•¸æ“š
    stats = {
        'total_items': len(data),
        'items_with_published_date': 0,
        'items_with_crawled_date': 0,
        'published_dates': [],
        'crawled_dates': [],
        'date_patterns': Counter(),
        'content_types': Counter(),
        'date_extraction_success_rate': 0,
    }
    
    for item in data:
        # æª¢æŸ¥ published_date
        published_date = item.get('published_date', '').strip()
        if published_date:
            stats['items_with_published_date'] += 1
            stats['published_dates'].append(published_date)
            
            # åˆ†ææ—¥æœŸæ ¼å¼æ¨¡å¼
            if re.match(r'^\d{4}-\d{2}-\d{2}$', published_date):
                stats['date_patterns']['YYYY-MM-DD'] += 1
            elif re.match(r'^\d{4}/\d{1,2}/\d{1,2}$', published_date):
                stats['date_patterns']['YYYY/M/D'] += 1
            elif re.match(r'^\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥$', published_date):
                stats['date_patterns']['ä¸­æ–‡æ—¥æœŸ'] += 1
            else:
                stats['date_patterns']['å…¶ä»–æ ¼å¼'] += 1
        
        # æª¢æŸ¥ crawled_at
        crawled_at = item.get('crawled_at', '').strip()
        if crawled_at:
            stats['items_with_crawled_date'] += 1
            # æå–æ—¥æœŸéƒ¨åˆ†
            crawled_date = crawled_at.split('T')[0] if 'T' in crawled_at else crawled_at
            stats['crawled_dates'].append(crawled_date)
        
        # çµ±è¨ˆå…§å®¹é¡å‹
        content_type = item.get('content_type', 'unknown')
        stats['content_types'][content_type] += 1
    
    # è¨ˆç®—æˆåŠŸç‡
    if stats['total_items'] > 0:
        stats['date_extraction_success_rate'] = (stats['items_with_published_date'] / stats['total_items']) * 100
    
    return stats

def analyze_date_distribution(dates):
    """åˆ†ææ—¥æœŸåˆ†å¸ƒ"""
    if not dates:
        return {}
    
    # è§£ææ—¥æœŸ
    parsed_dates = []
    for date_str in dates:
        try:
            # å˜—è©¦ä¸åŒçš„æ—¥æœŸæ ¼å¼
            for fmt in ['%Y-%m-%d', '%Y/%m/%d', '%Yå¹´%mæœˆ%dæ—¥']:
                try:
                    dt = datetime.strptime(date_str, fmt)
                    parsed_dates.append(dt)
                    break
                except ValueError:
                    continue
        except:
            continue
    
    if not parsed_dates:
        return {}
    
    # çµ±è¨ˆåˆ†å¸ƒ
    parsed_dates.sort()
    
    # æŒ‰å¹´ä»½åˆ†çµ„
    yearly_counts = defaultdict(int)
    monthly_counts = defaultdict(int)
    
    for dt in parsed_dates:
        yearly_counts[dt.year] += 1
        monthly_counts[f"{dt.year}-{dt.month:02d}"] += 1
    
    # è¨ˆç®—æ™‚é–“ç¯„åœ
    min_date = parsed_dates[0]
    max_date = parsed_dates[-1]
    date_range = (max_date - min_date).days
    
    return {
        'count': len(parsed_dates),
        'min_date': min_date.strftime('%Y-%m-%d'),
        'max_date': max_date.strftime('%Y-%m-%d'),
        'date_range_days': date_range,
        'yearly_distribution': dict(yearly_counts),
        'monthly_distribution': dict(monthly_counts),
        'recent_30_days': len([dt for dt in parsed_dates if (datetime.now() - dt).days <= 30]),
        'recent_90_days': len([dt for dt in parsed_dates if (datetime.now() - dt).days <= 90]),
        'recent_365_days': len([dt for dt in parsed_dates if (datetime.now() - dt).days <= 365]),
    }

def print_analysis_report(stats, file_path):
    """æ‰“å°åˆ†æå ±å‘Š"""
    print(f"\nğŸ“‹ æ–‡ä»¶åˆ†æå ±å‘Š: {Path(file_path).name}")
    print("=" * 80)
    
    # åŸºæœ¬çµ±è¨ˆ
    print(f"ğŸ“Š åŸºæœ¬çµ±è¨ˆ:")
    print(f"   ç¸½é …ç›®æ•¸: {stats['total_items']:,}")
    print(f"   æœ‰ç™¼å¸ƒæ—¥æœŸ: {stats['items_with_published_date']:,} ({stats['date_extraction_success_rate']:.1f}%)")
    print(f"   æœ‰çˆ¬å–æ—¥æœŸ: {stats['items_with_crawled_date']:,}")
    
    # å…§å®¹é¡å‹åˆ†å¸ƒ
    print(f"\nğŸ“ å…§å®¹é¡å‹åˆ†å¸ƒ:")
    for content_type, count in stats['content_types'].most_common():
        percentage = (count / stats['total_items']) * 100
        print(f"   {content_type}: {count:,} ({percentage:.1f}%)")
    
    # æ—¥æœŸæ ¼å¼åˆ†æ
    if stats['date_patterns']:
        print(f"\nğŸ“… æ—¥æœŸæ ¼å¼åˆ†å¸ƒ:")
        for pattern, count in stats['date_patterns'].most_common():
            percentage = (count / stats['items_with_published_date']) * 100 if stats['items_with_published_date'] > 0 else 0
            print(f"   {pattern}: {count:,} ({percentage:.1f}%)")
    
    # ç™¼å¸ƒæ—¥æœŸåˆ†æ
    if stats['published_dates']:
        print(f"\nğŸ—“ï¸  ç™¼å¸ƒæ—¥æœŸåˆ†æ:")
        pub_analysis = analyze_date_distribution(stats['published_dates'])
        if pub_analysis:
            print(f"   æ—¥æœŸç¯„åœ: {pub_analysis['min_date']} åˆ° {pub_analysis['max_date']}")
            print(f"   æ™‚é–“è·¨åº¦: {pub_analysis['date_range_days']} å¤©")
            print(f"   æœ€è¿‘30å¤©: {pub_analysis['recent_30_days']:,} é …")
            print(f"   æœ€è¿‘90å¤©: {pub_analysis['recent_90_days']:,} é …")
            print(f"   æœ€è¿‘1å¹´: {pub_analysis['recent_365_days']:,} é …")
            
            # å¹´ä»½åˆ†å¸ƒ
            if pub_analysis['yearly_distribution']:
                print(f"\n   å¹´ä»½åˆ†å¸ƒ:")
                for year in sorted(pub_analysis['yearly_distribution'].keys(), reverse=True):
                    count = pub_analysis['yearly_distribution'][year]
                    print(f"     {year}: {count:,} é …")
    
    # çˆ¬å–æ—¥æœŸåˆ†æ
    if stats['crawled_dates']:
        print(f"\nğŸ• çˆ¬å–æ—¥æœŸåˆ†æ:")
        crawl_analysis = analyze_date_distribution(stats['crawled_dates'])
        if crawl_analysis:
            print(f"   çˆ¬å–ç¯„åœ: {crawl_analysis['min_date']} åˆ° {crawl_analysis['max_date']}")
            print(f"   çˆ¬å–è·¨åº¦: {crawl_analysis['date_range_days']} å¤©")

def suggest_date_filters(stats):
    """å»ºè­°åˆé©çš„æ—¥æœŸéæ¿¾æ¢ä»¶"""
    print(f"\nğŸ’¡ å»ºè­°çš„æ™‚é–“éæ¿¾è¨­å®š:")
    print("=" * 50)
    
    if stats['items_with_published_date'] == 0:
        print("âš ï¸  ç”±æ–¼ç™¼å¸ƒæ—¥æœŸæå–æˆåŠŸç‡ç‚º 0%ï¼Œå»ºè­°ï¼š")
        print("   1. æ”¹é€²æ™‚é–“æå–é‚è¼¯")
        print("   2. ä½¿ç”¨çˆ¬å–æ™‚é–“ä½œç‚ºæ›¿ä»£")
        print("   3. æš«æ™‚ä¸ä½¿ç”¨æ™‚é–“éæ¿¾")
        return
    
    success_rate = stats['date_extraction_success_rate']
    
    if success_rate < 10:
        print(f"âš ï¸  ç™¼å¸ƒæ—¥æœŸæå–æˆåŠŸç‡è¼ƒä½ ({success_rate:.1f}%)ï¼Œå»ºè­°ï¼š")
        print("   1. å…ˆæ”¹é€²æ™‚é–“æå–é‚è¼¯")
        print("   2. è¬¹æ…ä½¿ç”¨æ™‚é–“éæ¿¾")
    elif success_rate < 50:
        print(f"ğŸ”¶ ç™¼å¸ƒæ—¥æœŸæå–æˆåŠŸç‡ä¸­ç­‰ ({success_rate:.1f}%)ï¼Œå»ºè­°ï¼š")
        print("   1. å¯ä»¥ä½¿ç”¨å¯¬é¬†çš„æ™‚é–“éæ¿¾")
        print("   2. è€ƒæ…®æ”¹é€²æ™‚é–“æå–é‚è¼¯")
    else:
        print(f"âœ… ç™¼å¸ƒæ—¥æœŸæå–æˆåŠŸç‡è‰¯å¥½ ({success_rate:.1f}%)ï¼Œå¯ä»¥å®‰å…¨ä½¿ç”¨æ™‚é–“éæ¿¾")
    
    # åˆ†æç™¼å¸ƒæ—¥æœŸåˆ†å¸ƒä¸¦çµ¦å‡ºå»ºè­°
    if stats['published_dates']:
        pub_analysis = analyze_date_distribution(stats['published_dates'])
        if pub_analysis:
            print(f"\nğŸ“… æ ¹æ“šæ•¸æ“šåˆ†å¸ƒçš„å»ºè­°:")
            
            recent_30 = pub_analysis['recent_30_days']
            recent_90 = pub_analysis['recent_90_days']
            recent_365 = pub_analysis['recent_365_days']
            total = pub_analysis['count']
            
            if recent_30 > total * 0.5:
                print(f"   ğŸ”¥ å¤§éƒ¨åˆ†å…§å®¹éƒ½å¾ˆæ–° (50%+ åœ¨30å¤©å…§)")
                print(f"      å»ºè­°: --preset recent_only (æœ€è¿‘2é€±)")
            elif recent_90 > total * 0.7:
                print(f"   ğŸ“ˆ å…§å®¹è¼ƒæ–° (70%+ åœ¨90å¤©å…§)")
                print(f"      å»ºè­°: --preset last_month æˆ– --preset last_3_months")
            elif recent_365 > total * 0.8:
                print(f"   ğŸ“Š å…§å®¹ç›¸å°è¼ƒæ–° (80%+ åœ¨1å¹´å…§)")
                print(f"      å»ºè­°: --preset last_6_months æˆ– --preset this_year")
            else:
                print(f"   ğŸ“š å…§å®¹è·¨åº¦è¼ƒå¤§ï¼ŒåŒ…å«è¼ƒå¤šæ­·å²è³‡æ–™")
                print(f"      å»ºè­°: --min-date {pub_analysis['max_date'][:4]}-01-01 (å¾ä»Šå¹´é–‹å§‹)")
            
            print(f"\nğŸ¯ å…·é«”å‘½ä»¤å»ºè­°:")
            print(f"   # åªçˆ¬å–æœ€è¿‘å…§å®¹")
            print(f"   python run_itri_crawler_with_date_filter.py --preset recent_only")
            print(f"   ")
            print(f"   # çˆ¬å–æœ€è¿‘3å€‹æœˆ")
            print(f"   python run_itri_crawler_with_date_filter.py --preset last_3_months")
            print(f"   ")
            print(f"   # è‡ªå®šç¾©æ—¥æœŸç¯„åœ")
            print(f"   python run_itri_crawler_with_date_filter.py --min-date 2024-01-01")

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='åˆ†æ ITRI çˆ¬å–æ•¸æ“šçš„æ™‚é–“åˆ†å¸ƒ')
    parser.add_argument('--file', help='æŒ‡å®šè¦åˆ†æçš„ JSON æ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--all', action='store_true', help='åˆ†ææ‰€æœ‰æ‰¾åˆ°çš„ JSON æ–‡ä»¶')
    
    args = parser.parse_args()
    
    print("ğŸ” ITRI çˆ¬å–æ•¸æ“šæ™‚é–“åˆ†æå·¥å…·")
    print("=" * 50)
    
    # ç¢ºå®šè¦åˆ†æçš„æ–‡ä»¶
    json_files = []
    
    if args.file:
        json_files = [Path(args.file)]
    else:
        # è‡ªå‹•æŸ¥æ‰¾ JSON æ–‡ä»¶
        base_dir = Path(__file__).parent
        
        # æœç´¢ crawled_data ç›®éŒ„
        crawled_data_dirs = [
            base_dir / "crawled_data",
            base_dir / "itri_scrapy_crawler" / "crawled_data"
        ]
        
        for crawled_dir in crawled_data_dirs:
            if crawled_dir.exists():
                json_files.extend(crawled_dir.rglob("*_articles.json"))
    
    if not json_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½• JSON æ•¸æ“šæ–‡ä»¶")
        print("ğŸ’¡ è«‹ç¢ºä¿å·²ç¶“é‹è¡Œéçˆ¬èŸ²ï¼Œæˆ–ä½¿ç”¨ --file æŒ‡å®šæ–‡ä»¶è·¯å¾‘")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(json_files)} å€‹æ•¸æ“šæ–‡ä»¶")
    
    # åˆ†ææ¯å€‹æ–‡ä»¶
    all_stats = []
    
    for json_file in json_files:
        if 'statistics' in json_file.name.lower():
            continue  # è·³éçµ±è¨ˆæ–‡ä»¶
            
        stats = analyze_dates_in_file(json_file)
        if stats:
            all_stats.append((json_file, stats))
            print_analysis_report(stats, json_file)
    
    # å¦‚æœæœ‰å¤šå€‹æ–‡ä»¶ï¼Œæä¾›ç¶œåˆå»ºè­°
    if len(all_stats) > 1:
        print(f"\nğŸ¯ ç¶œåˆåˆ†æå»ºè­°:")
        print("=" * 50)
        
        total_items = sum(stats['total_items'] for _, stats in all_stats)
        total_with_dates = sum(stats['items_with_published_date'] for _, stats in all_stats)
        overall_success_rate = (total_with_dates / total_items * 100) if total_items > 0 else 0
        
        print(f"ğŸ“Š ç¸½é«”çµ±è¨ˆ:")
        print(f"   ç¸½é …ç›®æ•¸: {total_items:,}")
        print(f"   æœ‰ç™¼å¸ƒæ—¥æœŸ: {total_with_dates:,} ({overall_success_rate:.1f}%)")
        
        # ä½¿ç”¨æœ€æ–°æ–‡ä»¶çš„çµ±è¨ˆé€²è¡Œå»ºè­°
        if all_stats:
            latest_file, latest_stats = max(all_stats, key=lambda x: x[0].stat().st_mtime)
            print(f"\nåŸºæ–¼æœ€æ–°æ–‡ä»¶ ({latest_file.name}) çš„å»ºè­°:")
            suggest_date_filters(latest_stats)
    elif len(all_stats) == 1:
        suggest_date_filters(all_stats[0][1])

if __name__ == "__main__":
    main()












