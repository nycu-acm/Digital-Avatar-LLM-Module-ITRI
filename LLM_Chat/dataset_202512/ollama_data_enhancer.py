#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ollama-Enhanced Data Processor for ITRI Dataset 2025
Uses Ollama LLM to intelligently clean, enhance, and structure web-crawled data.
"""

import json
import re
import requests
import time
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import hashlib
from datetime import datetime
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class EnhancedChunk:
    """Structure for LLM-enhanced data chunks"""
    chunk_id: str
    original_content: str
    cleaned_content: str
    structured_data: Dict[str, Any]
    summary: str
    key_points: List[str]
    entities: List[str]
    metadata: Dict[str, Any]
    language: str
    quality_score: float
    enhancement_log: List[str]

class OllamaDataEnhancer:
    def __init__(self, 
                 ollama_base_url: str = "http://localhost:11435",
                 model_name: str = "linly-llama3.1:70b-instruct-q4_0",
                 dataset_dir: str = "dataset_202512"):
        """Initialize the Ollama-enhanced data processor"""
        self.ollama_base_url = ollama_base_url
        self.model_name = model_name
        self.dataset_dir = Path(dataset_dir)
        
        # Test Ollama connection immediately 
        self._test_ollama_connection()
        
        # Enhancement configuration
        self.enhancement_config = {
            'max_retries': 3,
            'request_timeout': 120,  # 2 minutes timeout
            'batch_size': 5,  # Process multiple items together for efficiency
            'quality_threshold': 0.6,
            'min_content_length': 50,
            'max_content_length': 2000
        }
        
        logger.info(f"ğŸ¤– Ollama Data Enhancer initialized with model: {model_name}")

    def _test_ollama_connection(self):
        """Test connection to Ollama server and show detailed error if fails"""
        try:
            logger.info(f"ğŸ”— Testing connection to Ollama server at {self.ollama_base_url}")
            response = requests.get(f"{self.ollama_base_url}/api/tags", timeout=120)
            response.raise_for_status()
            
            # Also test if the model exists
            tags_data = response.json()
            models = [model.get('name', '') for model in tags_data.get('models', [])]
            
            if self.model_name in models:
                logger.info(f"âœ… Connected to Ollama server successfully")
                logger.info(f"âœ… Model {self.model_name} is available")
            else:
                logger.error(f"âŒ Model {self.model_name} not found")
                logger.error(f"Available models: {models}")
                raise ConnectionError(f"Model {self.model_name} not available. Run: ollama pull {self.model_name}")
                
        except requests.exceptions.ConnectionError as e:
            logger.error(f"âŒ Cannot connect to Ollama server at {self.ollama_base_url}")
            logger.error(f"Connection error: {e}")
            logger.error("Please ensure Ollama server is running:")
            logger.error("  ollama serve")
            raise ConnectionError("Ollama server not reachable")
            
        except requests.exceptions.Timeout as e:
            logger.error(f"âŒ Connection to Ollama server timed out")
            logger.error(f"Timeout error: {e}")
            raise ConnectionError("Ollama server timeout")
            
        except Exception as e:
            logger.error(f"âŒ Unexpected error connecting to Ollama: {e}")
            logger.error(f"Error type: {type(e).__name__}")
            raise ConnectionError(f"Ollama connection failed: {e}")

    def enhance_crawled_data(self, input_file: str) -> List[EnhancedChunk]:
        """Process crawled data with Ollama LLM enhancement"""
        logger.info(f"ğŸš€ Starting Ollama-enhanced processing of {input_file}")
        
        # Load crawled data
        input_path = self.dataset_dir / input_file
        with open(input_path, 'r', encoding='utf-8') as f:
            crawled_data = json.load(f)
        
        # Extract content items
        if isinstance(crawled_data, list):
            content_items = crawled_data
        elif isinstance(crawled_data, dict) and 'rag_ready_data' in crawled_data:
            content_items = crawled_data['rag_ready_data']
        else:
            content_items = [crawled_data]
        
        enhanced_chunks = []
        
        # Process items in batches for efficiency
        for i in range(0, len(content_items), self.enhancement_config['batch_size']):
            batch = content_items[i:i + self.enhancement_config['batch_size']]
            batch_results = self._process_batch(batch, i)
            enhanced_chunks.extend(batch_results)
            
            # Small delay to avoid overwhelming Ollama
            time.sleep(0.5)
        
        logger.info(f"âœ… Enhanced {len(content_items)} items into {len(enhanced_chunks)} high-quality chunks")
        return enhanced_chunks

    def _process_batch(self, batch: List[Dict[str, Any]], batch_index: int) -> List[EnhancedChunk]:
        """Process a batch of content items with Ollama"""
        logger.info(f"âš™ï¸ Processing batch {batch_index // self.enhancement_config['batch_size'] + 1}")
        
        enhanced_chunks = []
        
        for item in batch:
            try:
                enhanced_chunk = self._enhance_single_item(item)
                if enhanced_chunk:
                    enhanced_chunks.append(enhanced_chunk)
            except Exception as e:
                logger.error(f"âŒ Error processing item: {e}")
                continue
        
        return enhanced_chunks

    def _enhance_single_item(self, item: Dict[str, Any]) -> Optional[EnhancedChunk]:
        """Enhance a single content item using Ollama"""
        original_content = item.get('content', '').strip()
        
        if len(original_content) < self.enhancement_config['min_content_length']:
            return None
        
        # Truncate very long content
        if len(original_content) > self.enhancement_config['max_content_length']:
            original_content = original_content[:self.enhancement_config['max_content_length']] + "..."
        
        enhancement_log = []
        
        try:
            # Step 1: Clean and extract main content
            cleaned_content = self._clean_content_with_llm(original_content)
            enhancement_log.append("Content cleaned with LLM")
            
            # Step 2: Extract structured data
            structured_data = self._extract_structured_data(cleaned_content)
            enhancement_log.append("Structured data extracted")
            
            # Step 3: Generate summary
            summary = self._generate_summary(cleaned_content)
            enhancement_log.append("Summary generated")
            
            # Step 4: Extract key points
            key_points = self._extract_key_points(cleaned_content)
            enhancement_log.append("Key points extracted")
            
            # Step 5: Extract entities
            entities = self._extract_entities(cleaned_content)
            enhancement_log.append("Entities extracted")
            
            # Step 6: Calculate enhanced quality score
            quality_score = self._calculate_enhanced_quality_score(
                original_content, cleaned_content, structured_data, key_points
            )
            
            # Create enhanced chunk
            chunk_id = f"enhanced_{hashlib.md5(original_content.encode()).hexdigest()[:8]}"
            
            enhanced_chunk = EnhancedChunk(
                chunk_id=chunk_id,
                original_content=original_content,
                cleaned_content=cleaned_content,
                structured_data=structured_data,
                summary=summary,
                key_points=key_points,
                entities=entities,
                metadata={
                    **item.get('metadata', {}),
                    'original_source': item.get('source', 'unknown'),
                    'enhanced_at': datetime.now().isoformat(),
                    'enhancement_model': self.model_name,
                    'original_length': len(original_content),
                    'cleaned_length': len(cleaned_content),
                    'compression_ratio': len(cleaned_content) / len(original_content) if original_content else 0
                },
                language=self._detect_language(cleaned_content),
                quality_score=quality_score,
                enhancement_log=enhancement_log
            )
            
            return enhanced_chunk
            
        except Exception as e:
            logger.error(f"âŒ Error enhancing content: {e}")
            return None

    def _clean_content_with_llm(self, content: str) -> str:
        """Use Ollama to clean and extract main content from noisy web data"""
        system_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å…§å®¹æ¸…ç†å°ˆå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯å¾ç¶²é æŠ“å–çš„åŸå§‹å…§å®¹ä¸­æå–æœ€é‡è¦å’Œæœ€ç›¸é—œçš„ä¿¡æ¯ã€‚

è«‹åŸ·è¡Œä»¥ä¸‹ä»»å‹™ï¼š
1. ç§»é™¤å°èˆªå…ƒç´ ã€å»£å‘Šã€é‡è¤‡å…§å®¹å’Œç„¡é—œä¿¡æ¯
2. ä¿ç•™æ‰€æœ‰èˆ‡å·¥ç ”é™¢(ITRI)ç›¸é—œçš„é‡è¦å…§å®¹
3. ç¢ºä¿å…§å®¹çµæ§‹æ¸…æ™°ã€èªå¥å®Œæ•´
4. ä¿æŒåŸæœ‰çš„èªè¨€ï¼ˆä¸­æ–‡æˆ–è‹±æ–‡ï¼‰
5. å¦‚æœå…§å®¹å¤ªçŸ­æˆ–ç„¡é—œï¼Œè«‹å›æ‡‰"INSUFFICIENT_CONTENT"

åªè¿”å›æ¸…ç†å¾Œçš„ä¸»è¦å…§å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡‹æˆ–æ¨™è¨˜ã€‚"""

        user_prompt = f"è«‹æ¸…ç†ä»¥ä¸‹ç¶²é å…§å®¹ï¼Œæå–èˆ‡å·¥ç ”é™¢ç›¸é—œçš„æ ¸å¿ƒä¿¡æ¯ï¼š\n\n{content}"
        
        try:
            response = self._call_ollama(system_prompt, user_prompt)
            
            if response and response.strip() != "INSUFFICIENT_CONTENT":
                return response.strip()
            else:
                # Fallback to basic cleaning if LLM considers content insufficient
                return self._basic_content_cleaning(content)
                
        except Exception as e:
            logger.warning(f"âš ï¸ LLM cleaning failed, using basic cleaning: {e}")
            return self._basic_content_cleaning(content)

    def _extract_structured_data(self, content: str) -> Dict[str, Any]:
        """Extract structured data using Ollama"""
        system_prompt = """ä½ æ˜¯ä¸€å€‹æ•¸æ“šçµæ§‹åŒ–å°ˆå®¶ã€‚è«‹å¾çµ¦å®šçš„å…§å®¹ä¸­æå–çµæ§‹åŒ–ä¿¡æ¯ã€‚

è«‹ä»¥JSONæ ¼å¼è¿”å›ä»¥ä¸‹ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼š
- organization_info: çµ„ç¹”åŸºæœ¬ä¿¡æ¯
- key_people: é‡è¦äººç‰©
- achievements: æˆå°±æˆ–çé …
- technologies: æŠ€è¡“é ˜åŸŸ
- dates: é‡è¦æ—¥æœŸ
- locations: åœ°é»ä¿¡æ¯
- numbers: é‡è¦æ•¸å­—ï¼ˆå“¡å·¥æ•¸ã€ç‡Ÿæ”¶ç­‰ï¼‰

å¦‚æœæŸå€‹é¡åˆ¥æ²’æœ‰ä¿¡æ¯ï¼Œè«‹çœç•¥è©²å­—æ®µã€‚åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦å…¶ä»–è§£é‡‹ã€‚"""

        user_prompt = f"è«‹å¾ä»¥ä¸‹å…§å®¹ä¸­æå–çµæ§‹åŒ–ä¿¡æ¯ï¼š\n\n{content}"
        
        try:
            response = self._call_ollama(system_prompt, user_prompt)
            
            # Try to parse JSON response, but fall back to text extraction if JSON fails
            if response:
                # Clean up response - remove markdown formatting if present
                json_str = response.strip()
                if json_str.startswith('```json'):
                    json_str = json_str.replace('```json', '').replace('```', '').strip()
                
                try:
                    return json.loads(json_str)
                except json.JSONDecodeError:
                    # JSON parsing failed, return the raw LLM response directly
                    logger.info("ğŸ“Š JSON parsing failed, keeping raw LLM response")
                    return {"raw_llm_response": response}
            
        except Exception as e:
            logger.warning(f"âš ï¸ Structured data extraction failed: {e}")
            
        return {}
    

    def _generate_summary(self, content: str) -> str:
        """Generate a concise summary using Ollama"""
        system_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ‘˜è¦å°ˆå®¶ã€‚è«‹ç‚ºå·¥ç ”é™¢ç›¸é—œå…§å®¹ç”Ÿæˆç°¡æ½”è€Œå…¨é¢çš„æ‘˜è¦ã€‚

æ‘˜è¦è¦æ±‚ï¼š
1. é•·åº¦æ§åˆ¶åœ¨100-200å­—ä¹‹é–“
2. çªå‡ºæœ€é‡è¦çš„ä¿¡æ¯
3. ä¿æŒåŸæœ‰èªè¨€
4. ä½¿ç”¨æ¸…æ™°ç°¡æ½”çš„è¡¨é”

åªè¿”å›æ‘˜è¦å…§å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•å‰ç¶´æˆ–å¾Œç¶´ã€‚"""

        user_prompt = f"è«‹ç‚ºä»¥ä¸‹å…§å®¹ç”Ÿæˆæ‘˜è¦ï¼š\n\n{content}"
        
        try:
            response = self._call_ollama(system_prompt, user_prompt)
            return response.strip() if response else ""
        except Exception as e:
            logger.warning(f"âš ï¸ Summary generation failed: {e}")
            # Fallback to first few sentences
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', content)
            return 'ã€‚'.join(sentences[:2]) + 'ã€‚' if sentences else ""

    def _extract_key_points(self, content: str) -> List[str]:
        """Extract key points using Ollama"""
        system_prompt = """ä½ æ˜¯ä¸€å€‹ä¿¡æ¯æå–å°ˆå®¶ã€‚è«‹å¾å…§å®¹ä¸­æå–3-7å€‹æœ€é‡è¦çš„é—œéµé»ã€‚

è¦æ±‚ï¼š
1. æ¯å€‹é—œéµé»æ‡‰è©²ç°¡æ½”æ˜ç­ï¼ˆä¸è¶…é50å­—ï¼‰
2. é‡é»é—œæ³¨å·¥ç ”é™¢çš„æ ¸å¿ƒä¿¡æ¯
3. ä¿æŒåŸæœ‰èªè¨€
4. ä»¥åˆ—è¡¨æ ¼å¼è¿”å›ï¼Œæ¯è¡Œä¸€å€‹è¦é»
5. ä¸è¦æ·»åŠ ç·¨è™Ÿæˆ–ç¬¦è™Ÿï¼Œç›´æ¥åˆ—å‡ºè¦é»

åªè¿”å›é—œéµé»åˆ—è¡¨ï¼Œæ¯è¡Œä¸€å€‹ã€‚"""

        user_prompt = f"è«‹å¾ä»¥ä¸‹å…§å®¹ä¸­æå–é—œéµé»ï¼š\n\n{content}"
        
        try:
            response = self._call_ollama(system_prompt, user_prompt)
            
            if response:
                # Split response into lines and clean up
                key_points = [
                    line.strip().strip('-â€¢*').strip()
                    for line in response.split('\n')
                    if line.strip() and not line.strip().startswith('é—œéµé»')
                ]
                return [point for point in key_points if len(point) > 5][:7]  # Max 7 points
            
        except Exception as e:
            logger.warning(f"âš ï¸ Key points extraction failed: {e}")
            
        return []

    def _extract_entities(self, content: str) -> List[str]:
        """Extract named entities using Ollama"""
        system_prompt = """ä½ æ˜¯ä¸€å€‹å¯¦é«”è­˜åˆ¥å°ˆå®¶ã€‚è«‹å¾å…§å®¹ä¸­è­˜åˆ¥æ‰€æœ‰ç›¸é—œçš„å¯¦é«”ã€‚

è«‹è­˜åˆ¥ä»¥ä¸‹é¡å‹çš„å¯¦é«”ï¼š
1. äººç‰©å§“å
2. çµ„ç¹”æ©Ÿæ§‹
3. æŠ€è¡“åè©
4. åœ°é»
5. ç”¢å“æˆ–æœå‹™åç¨±
6. é‡è¦æ—¥æœŸæˆ–å¹´ä»½

åªè¿”å›å¯¦é«”åç¨±ï¼Œæ¯è¡Œä¸€å€‹ï¼Œä¸è¦åˆ†é¡æ¨™ç±¤ã€‚"""

        user_prompt = f"è«‹å¾ä»¥ä¸‹å…§å®¹ä¸­è­˜åˆ¥å¯¦é«”ï¼š\n\n{content}"
        
        try:
            response = self._call_ollama(system_prompt, user_prompt)
            
            if response:
                entities = [
                    line.strip()
                    for line in response.split('\n')
                    if line.strip() and len(line.strip()) > 1
                ]
                entities = list(set(entities))[:20]  # Remove duplicates, max 20 entities
                
                # If we got entities, return them
                if entities:
                    return entities
                    
        except Exception as e:
            logger.warning(f"âš ï¸ Entity extraction failed: {e}")
        
        # Fallback: extract entities using simple regex patterns
        logger.info("ğŸ”„ Using fallback entity extraction...")
        return self._extract_entities_fallback(content)
    
    def _extract_entities_fallback(self, content: str) -> List[str]:
        """Fallback entity extraction using regex patterns"""
        try:
            entities = []
            
            # Extract years (1900-2099)
            years = re.findall(r'\b(?:19|20)\d{2}å¹´?\b', content)
            entities.extend(years)
            
            # Extract names (Chinese names - 2-4 characters, proper format)
            chinese_names = re.findall(r'[è¶™éŒ¢å­«æå‘¨å³é„­ç‹é¦®é™³è¤šè¡›è”£æ²ˆéŸ“æ¥Šæœ±ç§¦å°¤è¨±ä½•å‘‚æ–½å¼µå­”æ›¹åš´è¯é‡‘é­é™¶å§œæˆšè¬é„’å–»æŸæ°´ç«‡ç« é›²è˜‡æ½˜è‘›å¥šèŒƒå½­éƒé­¯éŸ‹æ˜Œé¦¬è‹—é³³èŠ±æ–¹ä¿ä»»è¢æŸ³è±é®‘å²å”è²»å»‰å²‘è–›é›·è³€å€ªæ¹¯æ»•æ®·ç¾…ç•¢éƒé„”å®‰å¸¸æ¨‚æ–¼æ™‚å‚…çš®åé½Šåº·ä¼ä½™å…ƒåœé¡§å­Ÿå¹³é»ƒå’Œç©†è•­å°¹å§šé‚µå ªæ±ªç¥æ¯›ç¦¹ç‹„ç±³è²æ˜è‡§è¨ˆä¼æˆæˆ´è«‡å®‹èŒ…é¾ç†Šç´€èˆ’å±ˆé …ç¥è‘£][ä¸€-é¾¯]{1,3}', content)
            entities.extend(chinese_names)
            
            # Extract organizations (containing å…¬å¸, ç ”ç©¶æ‰€, å¤§å­¸, etc.)
            orgs = re.findall(r'[ä¸€-é¾¯\w]+(?:å…¬å¸|ç ”ç©¶æ‰€|å¤§å­¸|å­¸é™¢|ä¸­å¿ƒ|å”æœƒ|åŸºé‡‘æœƒ|é›†åœ˜)', content)
            entities.extend(orgs)
            
            # Extract technology terms
            tech_terms = re.findall(r'(?:äººå·¥æ™ºæ…§|AI|åŠå°é«”|ç”Ÿé†«|ç¶ èƒ½|5G|IoT|å€å¡Šéˆ|é›²ç«¯|å¤§æ•¸æ“š|æ©Ÿå™¨å­¸ç¿’)', content)
            entities.extend(tech_terms)
            
            # Extract locations (Taiwan cities and areas)
            locations = re.findall(r'(?:å°åŒ—|æ–°åŒ—|æ¡ƒåœ’|æ–°ç«¹|è‹—æ —|å°ä¸­|å½°åŒ–|å—æŠ•|é›²æ—|å˜‰ç¾©|å°å—|é«˜é›„|å±æ±|å®œè˜­|èŠ±è“®|å°æ±|æ¾æ¹–|é‡‘é–€|é€£æ±Ÿ|ç«¹åŒ—|ç«¹æ±|å…­ç”²)', content)
            entities.extend(locations)
            
            # Clean and deduplicate
            clean_entities = []
            for entity in entities:
                entity = entity.strip()
                if len(entity) > 1 and entity not in clean_entities:
                    clean_entities.append(entity)
            
            return clean_entities[:15]  # Return max 15 entities
            
        except Exception as e:
            logger.warning(f"âš ï¸ Fallback entity extraction failed: {e}")
            return []

    def _call_ollama(self, system_prompt: str, user_prompt: str) -> str:
        """Make a call to Ollama API with retry logic"""
        for attempt in range(self.enhancement_config['max_retries']):
            try:
                response = requests.post(
                    f"{self.ollama_base_url}/api/chat",
                    json={
                        "model": self.model_name,
                        "messages": [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        "stream": False,
                        "options": {
                            "temperature": 0.3,  # Lower temperature for more consistent results
                            "top_p": 0.9
                        }
                    },
                    timeout=self.enhancement_config['request_timeout']
                )
                response.raise_for_status()
                return response.json()['message']['content']
                
            except Exception as e:
                if attempt < self.enhancement_config['max_retries'] - 1:
                    logger.warning(f"âš ï¸ Ollama call failed (attempt {attempt + 1}), retrying: {e}")
                    time.sleep(2 ** attempt)  # Exponential backoff
                else:
                    logger.error(f"âŒ Ollama call failed after {self.enhancement_config['max_retries']} attempts: {e}")
                    raise

    def _basic_content_cleaning(self, content: str) -> str:
        """Fallback basic content cleaning"""
        # Remove common noise patterns
        noise_patterns = [
            r'Cookie.*?Policy',
            r'Privacy.*?Policy',
            r'Terms.*?of.*?Service',
            r'Skip to.*?content',
            r'Navigation.*?menu',
            r'Footer.*?links',
            r'Copyright.*?\d{4}',
            r'All rights reserved',
            r'Click here',
            r'Read more',
            r'Learn more'
        ]
        
        cleaned = content
        for pattern in noise_patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)
        
        # Clean up whitespace
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        
        return cleaned

    def _detect_language(self, text: str) -> str:
        """Detect language of the text"""
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        total_chars = len(text.replace(' ', ''))
        
        if total_chars == 0:
            return 'unknown'
        
        chinese_ratio = chinese_chars / total_chars
        return 'zh-tw' if chinese_ratio > 0.1 else 'en'

    def _calculate_enhanced_quality_score(self, original: str, cleaned: str, 
                                        structured_data: Dict, key_points: List[str]) -> float:
        """Calculate quality score for enhanced content"""
        scores = {}
        
        # Content improvement score (based on cleaning effectiveness)
        if len(original) > 0:
            compression_ratio = len(cleaned) / len(original)
            # Good compression (removing noise) should be between 0.3-0.8
            scores['content_improvement'] = min(1.0, max(0.0, 1.0 - abs(compression_ratio - 0.6) * 2))
        else:
            scores['content_improvement'] = 0.0
        
        # Structured data richness
        structured_count = sum(len(v) if isinstance(v, (list, dict, str)) else 1 for v in structured_data.values())
        scores['structured_richness'] = min(1.0, structured_count / 10.0)
        
        # Key points quality
        scores['key_points_quality'] = min(1.0, len(key_points) / 5.0)
        
        # Content length appropriateness
        length = len(cleaned)
        if 100 <= length <= 800:
            scores['length_quality'] = 1.0
        elif length < 100:
            scores['length_quality'] = length / 100.0
        else:
            scores['length_quality'] = max(0.3, 800.0 / length)
        
        # ITRI relevance (keyword-based)
        itri_keywords = ['å·¥ç ”é™¢', 'ITRI', 'ç ”ç™¼', 'æŠ€è¡“', 'å‰µæ–°', 'research', 'technology', 'innovation']
        keyword_matches = sum(1 for keyword in itri_keywords if keyword.lower() in cleaned.lower())
        scores['itri_relevance'] = min(1.0, keyword_matches / 3.0)
        
        # Weighted average
        weights = {
            'content_improvement': 0.25,
            'structured_richness': 0.2,
            'key_points_quality': 0.2,
            'length_quality': 0.15,
            'itri_relevance': 0.2
        }
        
        total_score = sum(scores[metric] * weight for metric, weight in weights.items())
        return round(total_score, 3)

    def save_enhanced_data(self, enhanced_chunks: List[EnhancedChunk], output_file: str):
        """Save enhanced data to file"""
        output_path = self.dataset_dir / output_file
        
        # Prepare data for saving
        enhanced_data = {
            'metadata': {
                'total_chunks': len(enhanced_chunks),
                'enhanced_at': datetime.now().isoformat(),
                'enhancement_model': self.model_name,
                'quality_distribution': self._analyze_quality_distribution(enhanced_chunks),
                'language_distribution': self._analyze_language_distribution(enhanced_chunks),
                'average_enhancement_ratio': self._calculate_average_enhancement_ratio(enhanced_chunks)
            },
            'chunks': [asdict(chunk) for chunk in enhanced_chunks]
        }
        
        # Save as JSON
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(enhanced_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… Saved {len(enhanced_chunks)} enhanced chunks to {output_path}")

    def _analyze_quality_distribution(self, chunks: List[EnhancedChunk]) -> Dict[str, Any]:
        """Analyze quality score distribution"""
        if not chunks:
            return {}
        
        quality_scores = [chunk.quality_score for chunk in chunks]
        
        return {
            'min_score': min(quality_scores),
            'max_score': max(quality_scores),
            'avg_score': sum(quality_scores) / len(quality_scores),
            'high_quality_count': len([s for s in quality_scores if s >= 0.7]),
            'medium_quality_count': len([s for s in quality_scores if 0.4 <= s < 0.7]),
            'low_quality_count': len([s for s in quality_scores if s < 0.4])
        }

    def _analyze_language_distribution(self, chunks: List[EnhancedChunk]) -> Dict[str, int]:
        """Analyze language distribution"""
        languages = {}
        for chunk in chunks:
            lang = chunk.language
            languages[lang] = languages.get(lang, 0) + 1
        return languages

    def _calculate_average_enhancement_ratio(self, chunks: List[EnhancedChunk]) -> float:
        """Calculate average content enhancement ratio"""
        if not chunks:
            return 0.0
        
        ratios = []
        for chunk in chunks:
            if chunk.original_content and chunk.cleaned_content:
                ratio = len(chunk.cleaned_content) / len(chunk.original_content)
                ratios.append(ratio)
        
        return sum(ratios) / len(ratios) if ratios else 0.0

    def generate_rag_ready_format(self, enhanced_chunks: List[EnhancedChunk], 
                                 quality_threshold: float = 0.6) -> List[Dict[str, Any]]:
        """Generate RAG-ready format from enhanced chunks"""
        
        # Filter by quality threshold
        high_quality_chunks = [chunk for chunk in enhanced_chunks if chunk.quality_score >= quality_threshold]
        
        logger.info(f"ğŸ“Š Filtered {len(enhanced_chunks)} chunks to {len(high_quality_chunks)} high-quality chunks (threshold={quality_threshold})")
        
        rag_ready_data = []
        
        for chunk in high_quality_chunks:
            # Use cleaned content as primary content, with summary as backup
            primary_content = chunk.cleaned_content if len(chunk.cleaned_content) > 50 else chunk.summary
            
            # Create enhanced metadata
            enhanced_metadata = {
                **chunk.metadata,
                'summary': chunk.summary,
                'key_points': chunk.key_points,
                'entities': chunk.entities,
                'structured_data': chunk.structured_data,
                'enhancement_log': chunk.enhancement_log,
                'quality_score': chunk.quality_score,
                'content_length': len(primary_content),
                'language': chunk.language
            }
            
            rag_item = {
                'content': primary_content,
                'chunk_id': chunk.chunk_id,
                'source_file': chunk.metadata.get('original_source', 'unknown'),
                'chunk_index': len(rag_ready_data),
                'metadata': enhanced_metadata
            }
            
            rag_ready_data.append(rag_item)
        
        return rag_ready_data

def main():
    """Main function for testing the Ollama data enhancer"""
    print("ğŸ¤– Ollama-Enhanced ITRI Data Processor")
    print("=" * 50)
    
    try:
        # Initialize enhancer
        enhancer = OllamaDataEnhancer()
        
        # Look for crawled data files
        dataset_dir = Path("dataset_202512")
        data_files = list(dataset_dir.glob("*rag_ready*.json"))
        
        if not data_files:
            print("âŒ No RAG-ready data files found. Please run the crawler first.")
            return
        
        # Process the most recent data file
        input_file = data_files[0].name
        print(f"ğŸ“Š Processing: {input_file}")
        
        # Enhance data with Ollama
        enhanced_chunks = enhancer.enhance_crawled_data(input_file)
        
        # Save enhanced data
        enhancer.save_enhanced_data(enhanced_chunks, "ollama_enhanced_data.json")
        
        # Generate RAG-ready format
        rag_ready_data = enhancer.generate_rag_ready_format(enhanced_chunks, quality_threshold=0.6)
        
        # Save final RAG-ready data
        final_output_path = dataset_dir / "ollama_enhanced_rag_ready.json"
        with open(final_output_path, 'w', encoding='utf-8') as f:
            json.dump(rag_ready_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Ollama enhancement completed!")
        print(f"ğŸ“Š Total enhanced chunks: {len(enhanced_chunks)}")
        print(f"ğŸ“Š High-quality RAG-ready chunks: {len(rag_ready_data)}")
        print(f"ğŸ“ Final output: {final_output_path}")
        
    except Exception as e:
        print(f"âŒ Enhancement failed: {e}")

if __name__ == "__main__":
    main()
