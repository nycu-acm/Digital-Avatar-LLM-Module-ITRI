#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ITRI Data Crawler - Step 1: Data Collection
ä½¿ç”¨ Scrapy çˆ¬å–å·¥ç ”é™¢ç›¸é—œè³‡æ–™ï¼ˆç¶­åŸºç™¾ç§‘ + å®˜ç¶²ï¼‰
"""

import os
import re
import scrapy
from scrapy.crawler import CrawlerProcess
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from scrapy.http import HtmlResponse
from datetime import datetime
from urllib.parse import parse_qs, urlparse

# Selenium æ”¯æŒï¼ˆå¯é¸ï¼‰
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, WebDriverException
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False
    print("âš ï¸  Selenium æœªå®‰è£ï¼Œå°‡ä½¿ç”¨æ¨™æº– HTTP è«‹æ±‚ã€‚å¦‚éœ€è™•ç†å‹•æ…‹ç¶²é ï¼Œè«‹å®‰è£: pip install selenium")

# --- è¨­å®šè¼¸å‡ºæª”æ¡ˆ ---
OUTPUT_DIR = "crawled_data"
OUTPUT_FILE = os.path.join(OUTPUT_DIR, "itri_raw_data.json")

# ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
os.makedirs(OUTPUT_DIR, exist_ok=True)

# å¦‚æœæª”æ¡ˆå·²å­˜åœ¨ï¼Œå…ˆå‚™ä»½
if os.path.exists(OUTPUT_FILE):
    backup_file = OUTPUT_FILE.replace(".json", f"_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    os.rename(OUTPUT_FILE, backup_file)
    print(f"ğŸ“¦ å·²å‚™ä»½èˆŠæª”æ¡ˆè‡³: {backup_file}")


class ItriWikiSpider(CrawlSpider):
    """ç¶­åŸºç™¾ç§‘å·¥ç ”é™¢ç›¸é—œé é¢çˆ¬èŸ²"""
    name = 'wiki_itri'
    allowed_domains = ['zh.wikipedia.org', 'en.wikipedia.org']
    
    # èµ·å§‹é é¢ï¼šå·¥ç ”é™¢ä¸»é é¢
    start_urls = [
        'https://zh.wikipedia.org/zh-tw/å·¥æ¥­æŠ€è¡“ç ”ç©¶é™¢',
        'https://en.wikipedia.org/wiki/Industrial_Technology_Research_Institute',
    ]
    
    # è¨­å®š BFS çˆ¬å–è¦å‰‡
    rules = (
        # å…è¨±ä¸­æ–‡å’Œè‹±æ–‡ç¶­åŸºé é¢ï¼Œæ’é™¤ç‰¹æ®Šé é¢ï¼ˆå¦‚ç·¨è¼¯é é¢ï¼‰
        Rule(
            LinkExtractor(
                allow=(r'/wiki/', r'/zh-tw/', r'/zh-cn/'),
                deny=(r':', r'Special:', r'User:', r'File:', r'Template:', r'Category:', r'Help:')
            ),
            callback='parse_item',
            follow=True
        ),
    )
    
    def parse(self, response):
        """è™•ç†èµ·å§‹ URLï¼Œä¹Ÿèª¿ç”¨ parse_item"""
        print(f"ğŸ” è§£æèµ·å§‹ URL: {response.url}")
        # å°æ–¼èµ·å§‹ URLï¼Œç›´æ¥èª¿ç”¨ parse_item
        yield from self.parse_item(response)
        # ç„¶å¾Œç¹¼çºŒè·Ÿéš¨é€£çµï¼ˆCrawlSpider æœƒè‡ªå‹•è™•ç†ï¼‰

    def parse_item(self, response):
        """è§£æç¶­åŸºç™¾ç§‘é é¢å…§å®¹"""
        # æŠ“å–æ¨™é¡Œ
        title = response.css('h1#firstHeading::text').get()
        if not title:
            title = response.css('h1.firstHeading::text').get()
        if not title:
            # å˜—è©¦å…¶ä»–é¸æ“‡å™¨
            title = response.css('h1::text').get()
        
        if not title:
            # å¦‚æœæ²’æœ‰æ¨™é¡Œï¼Œè·³éé€™å€‹é é¢
            return
        
        # æ’é™¤è¡¨æ ¼èˆ‡å°èˆªï¼ŒåªæŠ“ä¸»è¦æ®µè½
        # ä½¿ç”¨ XPath æå–æ‰€æœ‰æ–‡å­—ç¯€é»ï¼Œæ›´å¯é 
        content_list = response.xpath('//div[@id="mw-content-text"]//div[@class="mw-parser-output"]//p//text()').getall()
        if not content_list:
            # å˜—è©¦è‹±æ–‡ç‰ˆæ ¼å¼
            content_list = response.xpath('//div[@id="content"]//div[@class="mw-parser-output"]//p//text()').getall()
        if not content_list:
            # å˜—è©¦æ›´å¯¬é¬†çš„é¸æ“‡å™¨
            content_list = response.xpath('//div[@id="mw-content-text"]//p//text()').getall()
        if not content_list:
            # æœ€å¾Œå˜—è©¦ï¼šç›´æ¥å¾ body æå–æ®µè½
            content_list = response.xpath('//div[@id="bodyContent"]//p//text()').getall()
        
        # éæ¿¾å¤ªçŸ­çš„å…§å®¹ï¼ˆé™ä½é–€æª»ï¼‰
        content = " ".join([t.strip() for t in content_list if len(t.strip()) > 3])
        
        # å¦‚æœå…§å®¹å¤ªçŸ­ï¼Œå˜—è©¦æå–æ›´å¤šæ–‡å­—
        if len(content) < 50:
            # å˜—è©¦æå–æ‰€æœ‰æ®µè½æ–‡å­—ï¼ˆåŒ…æ‹¬æ›´çŸ­çš„ï¼‰
            all_text = response.xpath('//div[@id="mw-content-text"]//p//text()').getall()
            content = " ".join([t.strip() for t in all_text if len(t.strip()) > 2])
        
        # å¦‚æœé‚„æ˜¯æ²’æœ‰å…§å®¹ï¼Œå˜—è©¦å¾æ•´å€‹å…§å®¹å€åŸŸæå–
        if len(content) < 30:
            all_text = response.xpath('//div[@id="mw-content-text"]//text()').getall()
            content = " ".join([t.strip() for t in all_text if len(t.strip()) > 2])
            # éæ¿¾æ‰å°èˆªå’Œç„¡ç”¨æ–‡å­—
            content = re.sub(r'\[ç·¨è¼¯\]|\[edit\]', '', content)
        
        # æª¢æŸ¥æ˜¯å¦èˆ‡å·¥ç ”é™¢ç›¸é—œï¼ˆç°¡å–®é—œéµå­—æª¢æŸ¥ï¼‰
        # æ”¾å¯¬æ¢ä»¶ï¼šåªè¦æ¨™é¡Œæˆ–å…§å®¹ä¸­åŒ…å«é—œéµå­—å³å¯
        itri_keywords = ['å·¥ç ”é™¢', 'ITRI', 'Industrial Technology Research', 'å·¥æ¥­æŠ€è¡“ç ”ç©¶é™¢', 'å·¥æ¥­æŠ€è¡“', 'æŠ€è¡“ç ”ç©¶é™¢', 'Industrial Technology']
        title_content = (title or '') + ' ' + (content or '')
        is_relevant = any(keyword.lower() in title_content.lower() for keyword in itri_keywords)
        
        # å¦‚æœæ˜¯èµ·å§‹é é¢ï¼ˆå·¥ç ”é™¢ä¸»é é¢ï¼‰ï¼Œç„¡è«–å¦‚ä½•éƒ½è¦ä¿å­˜
        is_start_page = any(start_url in response.url for start_url in self.start_urls)
        
        # èª¿è©¦ä¿¡æ¯
        if is_start_page:
            print(f"ğŸ“„ è™•ç†èµ·å§‹é é¢: {response.url}")
            print(f"   æ¨™é¡Œ: {title}")
            print(f"   å…§å®¹é•·åº¦: {len(content)}")
        
        # æ”¾å¯¬æ¢ä»¶ï¼šèµ·å§‹é é¢æˆ–ç›¸é—œé é¢ï¼Œä¸”å…§å®¹é•·åº¦ > 20
        if (is_start_page or is_relevant) and content and len(content) > 20:
            print(f"âœ… ä¿å­˜ç¶­åŸºé é¢: {title} ({len(content)} å­—å…ƒ)")
            yield {
                'source': 'Wikipedia',
                'title': title.strip() if title else 'Untitled',
                'url': response.url,
                'content': content,
                'hierarchy': f"Wiki > {title.strip() if title else 'Untitled'}",
                'depth': response.meta.get('depth', 0),
                'language': 'zh-tw' if 'zh.wikipedia.org' in response.url else 'en',
                'crawled_at': datetime.now().isoformat()
            }
        else:
            # èª¿è©¦ä¿¡æ¯ï¼šç‚ºä»€éº¼æ²’æœ‰ä¿å­˜
            if is_start_page:
                print(f"âš ï¸  èµ·å§‹é é¢æœªä¿å­˜: å…§å®¹é•·åº¦={len(content)}, ç›¸é—œæ€§={is_relevant}")
            elif not is_relevant:
                print(f"âš ï¸  é é¢ä¸ç›¸é—œ: {title} (URL: {response.url})")
            elif not content or len(content) <= 20:
                print(f"âš ï¸  å…§å®¹å¤ªçŸ­: {title} (é•·åº¦: {len(content)})")


class ItriOfficialSpider(scrapy.Spider):
    """å·¥ç ”é™¢å®˜ç¶²çˆ¬èŸ²"""
    name = 'official_itri'
    allowed_domains = ['itri.org.tw', 'www.itri.org.tw']
    
    # å¾é¦–é å’Œç¶²ç«™åœ°åœ–é–‹å§‹
    start_urls = [
        'https://www.itri.org.tw/',
        'https://www.itri.org.tw/ListStyle.aspx?DisplayStyle=SiteMap&SiteID=1'
    ]

    def __init__(self, *args, **kwargs):
        super(ItriOfficialSpider, self).__init__(*args, **kwargs)
        self.visited_urls = set()
        self.max_pages = 200  # å¢åŠ é™åˆ¶ä»¥ç²å–æ›´å¤šé é¢
        self.use_selenium = kwargs.get('use_selenium', False) and SELENIUM_AVAILABLE
        
        # åˆå§‹åŒ– Seleniumï¼ˆå¦‚æœéœ€è¦ï¼‰
        self.driver = None
        if self.use_selenium:
            try:
                chrome_options = Options()
                chrome_options.add_argument('--headless')  # ç„¡é ­æ¨¡å¼
                chrome_options.add_argument('--no-sandbox')
                chrome_options.add_argument('--disable-dev-shm-usage')
                chrome_options.add_argument('--disable-gpu')
                chrome_options.add_argument('--window-size=1920,1080')
                chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
                
                self.driver = webdriver.Chrome(options=chrome_options)
                self.driver.set_page_load_timeout(60)  # å¢åŠ åˆ° 60 ç§’ï¼Œé¿å…è¶…æ™‚
                print("âœ… Selenium WebDriver å·²åˆå§‹åŒ–")
            except Exception as e:
                print(f"âš ï¸  Selenium åˆå§‹åŒ–å¤±æ•—: {e}ï¼Œå°‡ä½¿ç”¨æ¨™æº– HTTP è«‹æ±‚")
                self.use_selenium = False
                self.driver = None
    
    def closed(self, reason):
        """çˆ¬èŸ²é—œé–‰æ™‚æ¸…ç†è³‡æº"""
        if self.driver:
            try:
                self.driver.quit()
                print("âœ… Selenium WebDriver å·²é—œé–‰")
            except:
                pass

    def parse(self, response):
        """è§£æé¦–é æˆ–ç¶²ç«™åœ°åœ–"""
        if 'SiteMap' in response.url:
            # å¾ç¶²ç«™åœ°åœ–æŠ“å–æ‰€æœ‰é€£çµ
            links = response.css('.sitemap-list a::attr(href), .sitemap a::attr(href)').getall()
            for link in links[:self.max_pages]:
                if link and ('ListStyle' in link or 'Content' in link or 'News' in link):
                    full_url = response.urljoin(link)
                    if full_url not in self.visited_urls:
                        self.visited_urls.add(full_url)
                        yield response.follow(link, self.parse_detail, dont_filter=True)
        else:
            # å¾é¦–é æŠ“å–ä¸»è¦é€£çµ
            # å¦‚æœä½¿ç”¨ Seleniumï¼Œå…ˆç²å–å‹•æ…‹è¼‰å…¥çš„å…§å®¹
            if self.use_selenium and self.driver:
                try:
                    # è¨­ç½®æ›´é•·çš„è¶…æ™‚æ™‚é–“
                    self.driver.set_page_load_timeout(60)  # å¢åŠ åˆ° 60 ç§’
                    self.driver.get(response.url)
                    # ç­‰å¾…é é¢è¼‰å…¥å®Œæˆï¼ˆå¢åŠ è¶…æ™‚æ™‚é–“ï¼‰
                    WebDriverWait(self.driver, 30).until(
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )
                    # ç­‰å¾… JavaScript åŸ·è¡Œï¼ˆçµ¦ä¸€äº›æ™‚é–“è®“å‹•æ…‹å…§å®¹è¼‰å…¥ï¼‰
                    import time
                    time.sleep(3)  # å¢åŠ ç­‰å¾…æ™‚é–“
                    # ç²å–æ¸²æŸ“å¾Œçš„ HTML
                    html = self.driver.page_source
                    # å‰µå»ºæ–°çš„ response å°è±¡
                    response = HtmlResponse(url=response.url, body=html.encode('utf-8'), encoding='utf-8')
                except TimeoutException as e:
                    self.logger.warning(f"Selenium è¶…æ™‚ {response.url}: {e}ï¼Œä½¿ç”¨åŸå§‹ HTML")
                except Exception as e:
                    self.logger.warning(f"Selenium è™•ç†å¤±æ•— {response.url}: {e}ï¼Œä½¿ç”¨åŸå§‹ HTML")
            
            # ä½¿ç”¨ XPath å¾ HTML æºç¢¼ä¸­æå–æ‰€æœ‰é€£çµï¼ˆåŒ…æ‹¬å¯èƒ½è¢« CSS éš±è—çš„ï¼‰
            # é‡è¦ï¼šä½¿ç”¨æ›´å…¨é¢çš„ XPath ä¾†æå–æ‰€æœ‰é€£çµï¼Œä¸ç®¡å®ƒå€‘æ˜¯å¦è¢« CSS éš±è—
            processed_urls = set()
            
            # 1. å¾å°èˆªèœå–®ï¼ˆmega-menuï¼‰ä¸­æå–æ‰€æœ‰é€£çµï¼ˆåŒ…æ‹¬éš±è—çš„ï¼‰
            # ä½¿ç”¨æ›´å¯¬é¬†çš„é¸æ“‡å™¨ï¼Œç¢ºä¿èƒ½æå–åˆ°æ‰€æœ‰é€£çµ
            mega_menu_links = response.xpath(
                '//*[contains(@class, "mega-menu")]//a/@href | '
                '//*[contains(@class, "has-mega-menu")]//a/@href | '
                '//nav//a/@href | '
                '//ul[contains(@class, "mega-menu")]//a/@href | '
                '//li[contains(@class, "has-mega-menu")]//a/@href'
            ).getall()
            
            print(f"ğŸ” å¾ mega-menu æå–åˆ° {len(mega_menu_links)} å€‹é€£çµ")
            
            for link in mega_menu_links:
                if not link:
                    continue
                # ä¸è¦æ¸…ç†é€£çµï¼Œä¿ç•™å®Œæ•´ URLï¼ˆåŒ…æ‹¬åƒæ•¸ï¼‰
                full_url = response.urljoin(link)
                # é©—è­‰ URL æ˜¯å¦æœ‰æ•ˆï¼ˆæ’é™¤ç„¡æ•ˆåŸŸåï¼‰
                if self._is_valid_url(full_url):
                    # æ¨™æº–åŒ– URLï¼ˆç§»é™¤ç‰‡æ®µï¼Œä½†ä¿ç•™åƒæ•¸ï¼‰
                    url_normalized = full_url.split('#')[0]
                    if url_normalized not in processed_urls:
                        processed_urls.add(url_normalized)
                        self.visited_urls.add(url_normalized)
                        print(f"  âœ… æ·»åŠ é€£çµ: {url_normalized}")
                        yield response.follow(link, self.parse_detail, dont_filter=True)
            
            # 2. å¾æ‰€æœ‰é€£çµä¸­æå– ListStyle é€£çµï¼ˆä½¿ç”¨ XPath ç¢ºä¿èƒ½æå–åˆ°æ‰€æœ‰é€£çµï¼‰
            # é€™æœƒæå–é é¢ä¸­æ‰€æœ‰åŒ…å« ListStyle çš„é€£çµï¼Œä¸ç®¡å®ƒå€‘åœ¨å“ªè£¡
            all_liststyle_links = response.xpath('//a[contains(@href, "ListStyle")]/@href').getall()
            print(f"ğŸ” å¾æ‰€æœ‰é€£çµä¸­æå–åˆ° {len(all_liststyle_links)} å€‹ ListStyle é€£çµ")
            
            for link in all_liststyle_links:
                if not link:
                    continue
                full_url = response.urljoin(link)
                # é©—è­‰ URL æ˜¯å¦æœ‰æ•ˆ
                if self._is_valid_url(full_url):
                    # æ¨™æº–åŒ– URL
                    url_normalized = full_url.split('#')[0]
                    if url_normalized not in processed_urls:
                        processed_urls.add(url_normalized)
                        self.visited_urls.add(url_normalized)
                        print(f"  âœ… æ·»åŠ é€£çµ: {url_normalized}")
                        yield response.follow(link, self.parse_detail, dont_filter=True)
            
            # 3. ä¹Ÿæå– Content é€£çµ
            all_content_links = response.xpath('//a[contains(@href, "Content")]/@href').getall()
            print(f"ğŸ” å¾æ‰€æœ‰é€£çµä¸­æå–åˆ° {len(all_content_links)} å€‹ Content é€£çµ")
            
            for link in all_content_links:
                if not link:
                    continue
                full_url = response.urljoin(link)
                if self._is_valid_url(full_url):
                    url_normalized = full_url.split('#')[0]
                    if url_normalized not in processed_urls:
                        processed_urls.add(url_normalized)
                        self.visited_urls.add(url_normalized)
                        print(f"  âœ… æ·»åŠ é€£çµ: {url_normalized}")
                        yield response.follow(link, self.parse_detail, dont_filter=True)
    
    def _is_valid_url(self, url):
        """é©—è­‰ URL æ˜¯å¦æœ‰æ•ˆï¼ˆæ’é™¤ç„¡æ•ˆåŸŸåï¼‰"""
        try:
            parsed = urlparse(url)
            # æ’é™¤ç„¡æ•ˆåŸŸå
            invalid_domains = ['itriwww.itri.org.tw', 'itriwww.org.tw']  # éŒ¯èª¤çš„åŸŸå
            if parsed.netloc in invalid_domains:
                return False
            # åªå…è¨±æ­£ç¢ºçš„åŸŸåï¼ˆå…è¨±ç©º netlocï¼Œè¡¨ç¤ºç›¸å°è·¯å¾‘ï¼‰
            if parsed.netloc:
                valid_domains = ['www.itri.org.tw', 'itri.org.tw']
                if parsed.netloc not in valid_domains and not any(d in parsed.netloc for d in valid_domains):
                    return False
            # ç¢ºä¿æ˜¯ ListStyle æˆ– Content é€£çµ
            if 'ListStyle' not in url and 'Content' not in url:
                return False
            return True
        except:
            return False

    def parse_detail(self, response):
        """è§£æè©³ç´°é é¢å…§å®¹"""
        # å¦‚æœä½¿ç”¨ Seleniumï¼Œå…ˆç²å–å‹•æ…‹è¼‰å…¥çš„å…§å®¹
        if self.use_selenium and self.driver:
            try:
                # è¨­ç½®æ›´é•·çš„è¶…æ™‚æ™‚é–“
                self.driver.set_page_load_timeout(60)  # å¢åŠ åˆ° 60 ç§’
                self.driver.get(response.url)
                # ç­‰å¾…é é¢è¼‰å…¥å®Œæˆï¼ˆå¢åŠ è¶…æ™‚æ™‚é–“ï¼‰
                WebDriverWait(self.driver, 30).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                # ç­‰å¾… JavaScript åŸ·è¡Œï¼ˆçµ¦ä¸€äº›æ™‚é–“è®“å‹•æ…‹å…§å®¹è¼‰å…¥ï¼‰
                import time
                time.sleep(3)  # å¢åŠ ç­‰å¾…æ™‚é–“
                # ç²å–æ¸²æŸ“å¾Œçš„ HTML
                html = self.driver.page_source
                # å‰µå»ºæ–°çš„ response å°è±¡
                response = HtmlResponse(url=response.url, body=html.encode('utf-8'), encoding='utf-8')
            except TimeoutException as e:
                self.logger.warning(f"Selenium è¶…æ™‚ {response.url}: {e}ï¼Œä½¿ç”¨åŸå§‹ HTML")
            except Exception as e:
                self.logger.warning(f"Selenium è™•ç†å¤±æ•— {response.url}: {e}ï¼Œä½¿ç”¨åŸå§‹ HTML")
        # å˜—è©¦å¤šç¨®æ¨™é¡Œé¸æ“‡å™¨
        # å„ªå…ˆæå–è©³ç´°é é¢çš„æ¨™é¡Œï¼ˆh3 span#spanTitleï¼‰
        title = (
            response.css('h3 span#spanTitle::text, h3#spanTitle::text').get() or
            response.css('h2.title::text, h1.title::text, h1::text, h2::text').get() or
            response.css('.article-title::text, .content-title::text').get() or
            response.css('.page-title::text, .title-text::text').get() or
            response.css('title::text').get()
        )
        
        # æ”¹é€² breadcrumb æå–ï¼ˆåŒ…æ‹¬æœ€å¾Œçš„ç´”æ–‡å­—éƒ¨åˆ†ï¼‰
        breadcrumb_parts = []
        breadcrumb = response.css('.breadcrumb, #divBreadcrumb')
        if breadcrumb:
            # æ–¹æ³•1ï¼šæå–æ‰€æœ‰é€£çµæ–‡å­—ï¼ˆæŒ‰é †åºï¼‰
            breadcrumb_links = breadcrumb.css('a')
            for link in breadcrumb_links:
                # æå–é€£çµæ–‡å­—ï¼Œæ’é™¤åœ–æ¨™
                link_texts = link.css('::text').getall()
                link_text = ' '.join([t.strip() for t in link_texts if t.strip() and 'icon-' not in t])
                if link_text and len(link_text) > 1:
                    breadcrumb_parts.append(link_text.strip())
            
            # æ–¹æ³•2ï¼šæå– breadcrumb çš„å®Œæ•´æ–‡å­—å…§å®¹ï¼Œç„¶å¾Œæ‰¾å‡ºæœ€å¾Œçš„ç´”æ–‡å­—éƒ¨åˆ†
            breadcrumb_full_text = breadcrumb.get()
            if breadcrumb_full_text:
                # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æå–æ‰€æœ‰æ–‡å­—ï¼ˆåŒ…æ‹¬é€£çµå…§å’Œé€£çµå¤–çš„ï¼‰
                # å…ˆæå–æ‰€æœ‰é€£çµæ–‡å­—
                link_texts_in_html = re.findall(r'<a[^>]*>([^<]*(?:<i[^>]*>.*?</i>[^<]*)*)</a>', breadcrumb_full_text)
                for link_text in link_texts_in_html:
                    # ç§»é™¤åœ–æ¨™æ¨™ç±¤
                    link_text_clean = re.sub(r'<i[^>]*>.*?</i>', '', link_text).strip()
                    if link_text_clean and link_text_clean not in breadcrumb_parts:
                        breadcrumb_parts.append(link_text_clean)
                
                # ç§»é™¤æ‰€æœ‰é€£çµæ¨™ç±¤ï¼Œæå–å‰©é¤˜çš„ç´”æ–‡å­—
                text_without_links = re.sub(r'<a[^>]*>.*?</a>', '', breadcrumb_full_text)
                # ç§»é™¤æ‰€æœ‰ HTML æ¨™ç±¤
                text_without_tags = re.sub(r'<[^>]+>', '', text_without_links)
                # æå–éç©ºç™½æ–‡å­—
                remaining_texts = [t.strip() for t in text_without_tags.split() if t.strip() and len(t.strip()) > 1]
                for text in remaining_texts:
                    if text not in breadcrumb_parts:
                        breadcrumb_parts.append(text)
        
        # æå–é é¢ä¸»æ¨™é¡Œï¼ˆè©³ç´°é é¢çš„æ¨™é¡Œï¼Œå¦‚ h3 span#spanTitleï¼‰
        page_title = response.css('h3 span#spanTitle::text, h3#spanTitle::text').get()
        if page_title:
            page_title = page_title.strip()
        
        # çµ„åˆæ¨™é¡Œï¼šbreadcrumb + é é¢æ¨™é¡Œ
        if breadcrumb_parts:
            if page_title and page_title not in breadcrumb_parts:
                # å¦‚æœ breadcrumb æœ€å¾Œä¸€é …ä¸æ˜¯é é¢æ¨™é¡Œï¼Œå‰‡æ·»åŠ é é¢æ¨™é¡Œ
                title = ' > '.join(breadcrumb_parts + [page_title]) if page_title else ' > '.join(breadcrumb_parts)
            else:
                title = ' > '.join(breadcrumb_parts)
        elif page_title:
            title = page_title
        elif not title or not title.strip():
            # å˜—è©¦å¾å…¶ä»–å°èˆªä¸­æå–
            nav_title = response.css('.nav-current::text').getall()
            if nav_title:
                title = ' > '.join([t.strip() for t in nav_title if t.strip()])
            else:
                # å¾ URL åƒæ•¸æå–ï¼ˆå¦‚æœæœ‰ï¼‰
                parsed = urlparse(response.url)
                params = parse_qs(parsed.query)
                if 'MmmID' in params or 'DisplayStyle' in params:
                    title = "å·¥ç ”é™¢é é¢"  # é è¨­æ¨™é¡Œ
        
        # å¦‚æœé‚„æ˜¯æ²’æœ‰ï¼Œä½¿ç”¨é é¢æ¨™é¡Œ
        if not title or not title.strip():
            title = response.css('title::text').get() or "å·¥ç ”é™¢é é¢"
        
        title = title.strip() if title else "å·¥ç ”é™¢é é¢"
        
        # æ”¶é›†æ‰€æœ‰å¯èƒ½çš„æ–‡å­—å…§å®¹
        content_parts = []
        
        # é¦–å…ˆæå–æ—¥æœŸä¿¡æ¯ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
        pub_date = response.css('#pubDate::text, p.Lb#pubDate::text').get()
        if pub_date:
            pub_date = pub_date.strip()
            if pub_date and len(pub_date) > 3:
                content_parts.append(pub_date)
        
        # 0. ç‰¹åˆ¥è™•ç† #divContent å€å¡Šï¼ˆå·¥ç ”é™¢ç¶²ç«™å¸¸ç”¨ï¼‰
        div_content = response.css('#divContent')
        if div_content:
            # é¦–å…ˆæª¢æŸ¥ URLï¼šå¦‚æœåŒ…å« DisplayStyle=01_contentï¼Œé€™æ˜¯è©³ç´°å…§å®¹é é¢ï¼Œä¸æ˜¯ç›®éŒ„é 
            is_detail_page = 'DisplayStyle=01_content' in response.url or 'DisplayStyle=01%5Fcontent' in response.url
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºç›®éŒ„é ï¼ˆListStyle é é¢ï¼ŒåŒ…å«æ–°èåˆ—è¡¨ï¼‰
            # è­˜åˆ¥æ¨™èªŒï¼šæœ‰ <dl class="Bb_dotted pic_list"> æˆ–é¡ä¼¼çš„åˆ—è¡¨çµæ§‹
            # ä½†å¦‚æœæ˜¯è©³ç´°å…§å®¹é é¢ï¼Œå‰‡ä¸è¦–ç‚ºç›®éŒ„é 
            is_list_page = False
            if not is_detail_page:
                is_list_page = div_content.css('dl.Bb_dotted.pic_list, dl.pic_list, .pic_list').get() is not None
            
            if is_list_page:
                # é€™æ˜¯ç›®éŒ„é ï¼Œå°‡æ¯å€‹é …ç›®ä½œç‚ºç¨ç«‹æ¢ç›® yield
                # æå–é é¢ä¸»æ¨™é¡Œï¼ˆå¦‚æœæœ‰ï¼‰
                page_main_title = response.css('h3 span#spanTitle::text, h3::text').get()
                page_main_title = page_main_title.strip() if page_main_title else ""
                
                # æå–æ¯å€‹åˆ—è¡¨é …ç›®çš„æ¨™é¡Œå’Œç°¡ä»‹
                list_items = div_content.css('dl.Bb_dotted.pic_list dd, dl.pic_list dd, .pic_list dd')
                
                # å¦‚æœæ‰¾åˆ°åˆ—è¡¨é …ç›®ï¼Œæ¯å€‹é …ç›®ä½œç‚ºç¨ç«‹æ¢ç›®
                if list_items:
                    for idx, item in enumerate(list_items):
                        # æå–æ¨™é¡Œ
                        item_title = item.css('a.title::text, a[class*="title"]::text').get()
                        if not item_title:
                            item_title = item.css('a::attr(title)').get()
                        
                        # æå–ç°¡ä»‹
                        item_desc = item.css('p::text').get()
                        
                        # æå–åœ–ç‰‡ altï¼ˆå¦‚æœæœ‰ï¼‰
                        item_img_alt = item.xpath('./preceding-sibling::dt[1]//img/@alt').get()
                        
                        # æå–é€£çµ URL
                        item_url = item.css('a::attr(href)').get()
                        if item_url:
                            item_url = response.urljoin(item_url)
                        
                        if item_title:
                            # æ§‹å»ºè©²é …ç›®çš„å®Œæ•´æ¨™é¡Œï¼ˆåŒ…å«é é¢è·¯å¾‘ï¼‰
                            item_full_title = f"{title} > {item_title.strip()}" if title else item_title.strip()
                            
                            # æ§‹å»ºå…§å®¹
                            item_content_parts = []
                            if item_desc and item_desc.strip():
                                item_content_parts.append(item_desc.strip())
                            if item_img_alt and item_img_alt.strip():
                                item_content_parts.append(f"åœ–ç‰‡èªªæ˜: {item_img_alt.strip()}")
                            
                            item_content = "\n".join(item_content_parts) if item_content_parts else item_title.strip()
                            
                            # Yield æ¯å€‹é …ç›®ä½œç‚ºç¨ç«‹æ¢ç›®
                            yield {
                                'source': 'ITRI_Official',
                                'title': item_full_title,
                                'url': item_url or response.url,
                                'content': item_content,
                                'hierarchy': f"ITRI > Official > {item_full_title}",
                                'depth': 1,
                                'language': 'zh-tw',
                                'crawled_at': datetime.now().isoformat(),
                                'item_type': 'list_item',  # æ¨™è¨˜ç‚ºåˆ—è¡¨é …ç›®
                                'parent_page': title  # è¨˜éŒ„çˆ¶é é¢
                            }
                    
                    # ç›®éŒ„é çš„é …ç›®å·²ç¶“ä½œç‚ºç¨ç«‹æ¢ç›® yieldï¼Œä¸éœ€è¦ç¹¼çºŒè™•ç†
                    # ä¸ returnï¼Œè®“å‡½æ•¸è‡ªç„¶çµæŸï¼ˆé¿å… Scrapy è­¦å‘Šï¼‰
                
                # å¦‚æœæ²’æœ‰æ‰¾åˆ°åˆ—è¡¨é …ç›®ï¼Œå˜—è©¦å‚™ç”¨æ–¹æ¡ˆ
                list_links = div_content.css('a.title, a[class*="title"]')
                if list_links:
                    for link in list_links:
                        link_title = link.css('::text').get() or link.css('::attr(title)').get()
                        link_url = link.css('::attr(href)').get()
                        if link_url:
                            link_url = response.urljoin(link_url)
                        
                        if link_title and link_title.strip():
                            item_full_title = f"{title} > {link_title.strip()}" if title else link_title.strip()
                            yield {
                                'source': 'ITRI_Official',
                                'title': item_full_title,
                                'url': link_url or response.url,
                                'content': link_title.strip(),  # åªæœ‰æ¨™é¡Œ
                                'hierarchy': f"ITRI > Official > {item_full_title}",
                                'depth': 1,
                                'language': 'zh-tw',
                                'crawled_at': datetime.now().isoformat(),
                                'item_type': 'list_item',
                                'parent_page': title
                            }
                    
                    # ç›®éŒ„é è™•ç†å®Œæˆ
                    # ä¸ returnï¼Œè®“å‡½æ•¸è‡ªç„¶çµæŸï¼ˆé¿å… Scrapy è­¦å‘Šï¼‰
            else:
                # ä¸æ˜¯ç›®éŒ„é ï¼Œæ­£å¸¸è™•ç†
                # å„ªå…ˆæå– div.run_around ä¸­çš„å…§å®¹ï¼ˆé€™æ˜¯æœ€é‡è¦çš„å…§å®¹ï¼‰
                run_around = div_content.css('div.run_around')
                if run_around:
                    # æ–¹æ³•1ï¼šä½¿ç”¨ XPath æå–æ‰€æœ‰æ–‡å­—ç¯€é»
                    run_around_texts = run_around.xpath('.//text()').getall()
                    if run_around_texts:
                        # åˆä½µæ–‡å­—ï¼Œä¿ç•™çµæ§‹
                        run_around_content = ' '.join([t.strip() for t in run_around_texts if t.strip()])
                        # æ¸…ç†å¤šé¤˜çš„ç©ºç™½
                        run_around_content = re.sub(r'\s+', ' ', run_around_content).strip()
                        if run_around_content and len(run_around_content) > 10:
                            content_parts.append(run_around_content)
                    
                    # æ–¹æ³•2ï¼šå¦‚æœæ–¹æ³•1æ²’æœ‰è¶³å¤ å…§å®¹ï¼Œä½¿ç”¨ HTML è§£æ
                    if not content_parts or len(content_parts[-1]) < 50:
                        run_around_html = run_around.get()
                        if run_around_html:
                            # å°‡ <br> å’Œ <br/> æ›¿æ›ç‚ºæ›è¡Œç¬¦
                            run_around_html = re.sub(r'<br\s*/?>', '\n', run_around_html, flags=re.IGNORECASE)
                            # ç§»é™¤æ‰€æœ‰ HTML æ¨™ç±¤
                            run_around_text = re.sub(r'<[^>]+>', '', run_around_html)
                            # æ¸…ç†å¤šé¤˜çš„ç©ºç™½ï¼Œä½†ä¿ç•™æ›è¡Œ
                            lines = [line.strip() for line in run_around_text.split('\n') if line.strip()]
                            run_around_content = '\n'.join(lines)
                            if run_around_content and len(run_around_content) > 10:
                                # å¦‚æœå·²æœ‰å…§å®¹ï¼Œæ¯”è¼ƒé•·åº¦ï¼Œä¿ç•™è¼ƒé•·çš„
                                if content_parts and len(run_around_content) > len(content_parts[-1]):
                                    content_parts[-1] = run_around_content
                                elif not content_parts:
                                    content_parts.append(run_around_content)
                
                # æå–æ‰€æœ‰æ¨™é¡Œï¼ˆh4, h5, h6ï¼‰
                headings = div_content.css('h4, h5, h6')
                for heading in headings:
                    heading_text = ' '.join(heading.css('::text').getall()).strip()
                    if heading_text and len(heading_text) > 2:
                        content_parts.append(f"æ¨™é¡Œ: {heading_text}")
                
                # æå–æ‰€æœ‰æ®µè½æ–‡å­—ï¼ˆä½¿ç”¨ XPath ä»¥æ­£ç¢ºè™•ç† <br/> æ¨™ç±¤ï¼‰
                # ä½†æ’é™¤å·²ç¶“åœ¨ run_around ä¸­è™•ç†éçš„æ®µè½
                paragraphs = div_content.css('p')
                for para in paragraphs:
                    # æª¢æŸ¥æ˜¯å¦åœ¨ run_around ä¸­ï¼ˆé¿å…é‡è¤‡ï¼‰
                    in_run_around = para.xpath('./ancestor::div[@class="run_around"]').get()
                    if in_run_around:
                        continue
                    
                    # ä½¿ç”¨ XPath æå–æ‰€æœ‰æ–‡å­—ç¯€é»ï¼ˆåŒ…æ‹¬ <br/> å¾Œçš„æ–‡å­—ï¼‰
                    para_html = para.get()
                    if para_html:
                        # å°‡ <br> å’Œ <br/> æ›¿æ›ç‚ºæ›è¡Œç¬¦
                        para_html = re.sub(r'<br\s*/?>', '\n', para_html, flags=re.IGNORECASE)
                        # ç§»é™¤æ‰€æœ‰ HTML æ¨™ç±¤
                        para_text = re.sub(r'<[^>]+>', '', para_html)
                        # æ¸…ç†å¤šé¤˜çš„ç©ºç™½ï¼Œä½†ä¿ç•™æ›è¡Œ
                        lines = [line.strip() for line in para_text.split('\n') if line.strip()]
                        para_text = '\n'.join(lines)
                        if para_text and len(para_text) > 10:
                            # æª¢æŸ¥æ˜¯å¦å·²ç¶“åœ¨å…§å®¹ä¸­ï¼ˆé¿å…é‡è¤‡ï¼‰
                            if not any(para_text in part or part in para_text for part in content_parts if len(part) > 20):
                                content_parts.append(para_text)
                
                # æå–è¯çµ¡äººä¿¡æ¯ï¼ˆ.connection å€å¡Šï¼‰
                connection_blocks = div_content.css('.connection, .connection Lb')
                for conn in connection_blocks:
                    conn_texts = conn.xpath('.//text()').getall()
                    conn_content = ' '.join([t.strip() for t in conn_texts if t.strip()])
                    if conn_content and len(conn_content) > 5:
                        # æª¢æŸ¥å‰é¢æ˜¯å¦æœ‰æ¨™é¡Œï¼ˆå¦‚ã€Œã€æ–°èé€£çµ¡äººã€‘ã€ï¼‰
                        prev_h5 = conn.xpath('./preceding-sibling::h5[1]')
                        if not prev_h5.get():
                            prev_h5 = conn.xpath('./ancestor::p[1]/h5[1]')
                        if prev_h5.get():
                            h5_text = ' '.join(prev_h5.css('::text').getall()).strip()
                            if h5_text:
                                content_parts.append(f"{h5_text}\n{conn_content}")
                            else:
                                content_parts.append(conn_content)
                        else:
                            content_parts.append(conn_content)
            
            # ç‰¹åˆ¥è™•ç† .imglist å€å¡Šï¼ˆé™¢å£«åˆ—è¡¨ã€é …ç›®åˆ—è¡¨ç­‰ï¼‰
            img_lists = div_content.css('.imglist')
            for img_list in img_lists:
                # æ‰¾åˆ°é€™å€‹ imglist å‰é¢çš„æ¨™é¡Œï¼ˆh5ï¼‰
                prev_heading = img_list.xpath('./preceding-sibling::h5[1]')
                if not prev_heading.get():
                    prev_heading = img_list.xpath('./preceding-sibling::*[self::h4 or self::h5][1]')
                
                section_title = ""
                if prev_heading.get():
                    section_title = ' '.join(prev_heading.css('::text').getall()).strip()
                
                # æå–è©²å€å¡Šä¸­çš„æ‰€æœ‰é …ç›®
                items = []
                # å¾ figure ä¸­æå–
                figures_in_list = img_list.css('figure')
                for figure in figures_in_list:
                    # æå– figcaption ä¸­çš„ span æ–‡å­—ï¼ˆé™¢å£«åç¨±ç­‰ï¼‰
                    figcaption_spans = figure.css('figcaption span::text').getall()
                    figcaption_text = ' '.join([s.strip() for s in figcaption_spans if s.strip()])
                    
                    # å¦‚æœæ²’æœ‰ï¼Œå˜—è©¦å¾ a æ¨™ç±¤çš„ title æå–
                    if not figcaption_text:
                        link_title = figure.css('a::attr(title)').get()
                        if link_title:
                            figcaption_text = link_title.strip()
                    
                    if figcaption_text:
                        items.append(figcaption_text)
                
                # å¦‚æœæ²’æœ‰å¾ figure æå–åˆ°ï¼Œå˜—è©¦å¾ a æ¨™ç±¤æå–
                if not items:
                    links = img_list.css('a[title]')
                    for link in links:
                        link_title = link.css('::attr(title)').get()
                        if link_title and link_title.strip():
                            items.append(link_title.strip())
                
                # çµ„ç¹”æˆçµæ§‹åŒ–å…§å®¹
                if items:
                    if section_title:
                        content_parts.append(f"{section_title}:")
                    for item in items:
                        content_parts.append(f"  - {item}")
        
        # 1. æå–ç›¸é—œæ–°è/é …ç›®çš„æ¨™é¡Œå’Œæè¿°ï¼ˆé‡è¦ï¼ï¼‰
        # æå– figure å’Œ figcaption ä¸­çš„å…§å®¹ï¼ˆåªåœ¨ #divContent æˆ– #mainContent ä¸­ï¼‰
        figures = response.css('figure')
        for figure in figures:
            # åªæå–åœ¨ #divContent æˆ– #mainContent ä¸­çš„ figure
            in_content = figure.xpath('./ancestor-or-self::div[@id="divContent"] | ./ancestor-or-self::*[@id="mainContent"]').get()
            if not in_content:
                continue
            
            # æ’é™¤å°èˆªå€åŸŸ
            in_nav = figure.xpath('./ancestor::nav | ./ancestor::header | ./ancestor::*[contains(@class, "nav")] | ./ancestor::*[contains(@class, "menu")]').get()
            if in_nav:
                continue
            
            # æå– figcaption ä¸­çš„ span æ–‡å­—
            figcaption_spans = figure.css('figcaption span::text').getall()
            figcaption_text = ' '.join([s.strip() for s in figcaption_spans if s.strip()])
            
            if figcaption_text:
                content_parts.append(f"ç›¸é—œé …ç›®: {figcaption_text}")
            else:
                # å¦‚æœæ²’æœ‰ spanï¼Œå˜—è©¦ç›´æ¥æå– figcaption æ–‡å­—
                figcaption = figure.css('figcaption::text').get()
                if figcaption and figcaption.strip():
                    content_parts.append(f"ç›¸é—œé …ç›®: {figcaption.strip()}")
            
            # æå– a æ¨™ç±¤çš„ title å±¬æ€§
            link_title = figure.css('a::attr(title)').get()
            if link_title and link_title.strip() and link_title.strip() not in [item.split(': ')[-1] if ': ' in item else '' for item in content_parts]:
                content_parts.append(f"é …ç›®æ¨™é¡Œ: {link_title.strip()}")
            
            # æå–åœ–ç‰‡ alt æ–‡å­—
            img_alt = figure.css('img::attr(alt)').get()
            if img_alt and img_alt.strip():
                content_parts.append(f"åœ–ç‰‡èªªæ˜: {img_alt.strip()}")
        
        # æå–æ‰€æœ‰é€£çµçš„ title å±¬æ€§ï¼ˆç›¸é—œæ–°è/é …ç›®ï¼‰ï¼Œä½†æ’é™¤å·²ç¶“è™•ç†éçš„
        # åªæå–åœ¨ #divContent æˆ– #mainContent ä¸­çš„é€£çµ
        links = response.css('a[title]')
        processed_titles = set()
        for link in links:
            # åªæå–åœ¨ #divContent æˆ– #mainContent ä¸­çš„é€£çµ
            in_content = link.xpath('./ancestor-or-self::div[@id="divContent"] | ./ancestor-or-self::*[@id="mainContent"]').get()
            if not in_content:
                continue
            
            # æ’é™¤å°èˆªå€åŸŸ
            in_nav = link.xpath('./ancestor::nav | ./ancestor::header | ./ancestor::*[contains(@class, "nav")] | ./ancestor::*[contains(@class, "menu")] | ./ancestor::*[contains(@class, "mega-menu")]').get()
            if in_nav:
                continue
            
            link_title = link.css('::attr(title)').get()
            if link_title and link_title.strip() and len(link_title.strip()) > 5:
                # éæ¿¾æ‰å¸¸è¦‹çš„ç„¡ç”¨æ¨™é¡Œ
                if not any(skip in link_title for skip in ['å›ä¸Šä¸€é ', 'å›é ‚ç«¯', 'æ›´å¤š', 'More', 'javascript', 'å¦é–‹è¦–çª—', 'Share to']):
                    if link_title.strip() not in processed_titles:
                        processed_titles.add(link_title.strip())
                        # æª¢æŸ¥æ˜¯å¦å·²ç¶“åœ¨å…§å®¹ä¸­
                        if not any(link_title.strip() in item for item in content_parts):
                            content_parts.append(f"ç›¸é—œé€£çµ: {link_title.strip()}")
        
        # 2. æå–é é¢ç‰¹å®šå€åŸŸçš„å…§å®¹ï¼ˆç¢ºä¿æ¯å€‹é é¢éƒ½æœ‰ç¨ç‰¹å…§å®¹ï¼‰
        # å˜—è©¦æå–é é¢ä¸»æ¨™é¡Œä¸‹çš„å…§å®¹å€å¡Šï¼ˆä½†æ’é™¤å·²ç¶“è™•ç†éçš„ #divContent å’Œå°èˆªå€åŸŸï¼‰
        page_specific_selectors = [
            '.content-detail',
            '.detail-content',
            '.page-detail',
            '.main-detail',
            '.article-detail',
            '[class*="detail"]',
            '[class*="content"]:not(#divContent)',  # æ’é™¤å·²è™•ç†çš„
            '[id*="detail"]'
        ]
        
        for selector in page_specific_selectors:
            elements = response.css(selector)
            if elements:
                for element in elements:
                    # æª¢æŸ¥æ˜¯å¦åœ¨ #divContent ä¸­ï¼ˆé¿å…é‡è¤‡ï¼‰
                    ancestor_div = element.xpath('./ancestor-or-self::div[@id="divContent"]')
                    if ancestor_div.get():
                        continue
                    
                    # æ’é™¤å°èˆªå€åŸŸ
                    in_nav = element.xpath('./ancestor::nav | ./ancestor::header | ./ancestor::*[contains(@class, "nav")] | ./ancestor::*[contains(@class, "menu")] | ./ancestor::*[contains(@class, "mega-menu")]').get()
                    if in_nav:
                        continue
                    
                    # åªæå–åœ¨ #mainContent ä¸­çš„å…§å®¹
                    in_main = element.xpath('./ancestor-or-self::*[@id="mainContent"]').get()
                    if not in_main:
                        continue
                    
                    # æå–è©²å…ƒç´ çš„æ‰€æœ‰æ–‡å­—
                    text = element.css('::text').getall()
                    if text:
                        text_content = ' '.join([t.strip() for t in text if len(t.strip()) > 3])
                        if len(text_content) > 20:  # ç¢ºä¿æœ‰å¯¦è³ªå…§å®¹
                            content_parts.append(text_content)
        
        # 3. å…ˆå˜—è©¦å¸¸è¦‹çš„å…§å®¹å€å¡Šé¸æ“‡å™¨ï¼ˆä½†ä¸è¦ breakï¼Œç¹¼çºŒæ”¶é›†ï¼‰
        # æ’é™¤å·²ç¶“è™•ç†éçš„ #divContent å’Œå°èˆªå€åŸŸ
        content_selectors = [
            '.article-content',
            '.content-box',
            '.editor-content',
            '.main-content',
            '#content:not(#divContent)',  # æ’é™¤å·²è™•ç†çš„
            '.article-body',
            'article',
            '.content-area',
            '.page-content'
        ]
        
        for selector in content_selectors:
            elements = response.css(selector)
            if elements:
                for element in elements:
                    # æª¢æŸ¥æ˜¯å¦åœ¨ #divContent ä¸­ï¼ˆé¿å…é‡è¤‡ï¼‰
                    ancestor_div = element.xpath('./ancestor-or-self::div[@id="divContent"]')
                    if ancestor_div.get():
                        continue
                    
                    # æ’é™¤å°èˆªå€åŸŸ
                    in_nav = element.xpath('./ancestor::nav | ./ancestor::header | ./ancestor::*[contains(@class, "nav")] | ./ancestor::*[contains(@class, "menu")] | ./ancestor::*[contains(@class, "mega-menu")]').get()
                    if in_nav:
                        continue
                    
                    # åªæå–åœ¨ #mainContent ä¸­çš„å…§å®¹
                    in_main = element.xpath('./ancestor-or-self::*[@id="mainContent"]').get()
                    if not in_main:
                        continue
                    
                    # å¾é€™äº›å€å¡Šä¸­æå–æ‰€æœ‰æ–‡å­—
                    text = element.css('::text').getall()
                    if text:
                        text_content = ' '.join([t.strip() for t in text if len(t.strip()) > 3])
                        if len(text_content) > 20:
                            content_parts.append(text_content)
        
        # 4. å¦‚æœé‚„æ²’æœ‰è¶³å¤ å…§å®¹ï¼Œå˜—è©¦æŠ“å–è¡¨æ ¼å…§å®¹ï¼ˆé‡è¦ï¼ï¼‰
        # åªæå–åœ¨ #divContent æˆ– #mainContent ä¸­çš„è¡¨æ ¼
        if not content_parts or len(' '.join(content_parts).strip()) < 50:
            # ä½¿ç”¨ XPath æå–è¡¨æ ¼å…§å®¹ï¼ˆèƒ½æ›´å¥½åœ°è™•ç† <br> æ¨™ç±¤ï¼‰
            tables = response.xpath('//table')
            for table in tables:
                # åªæå–åœ¨ #divContent æˆ– #mainContent ä¸­çš„è¡¨æ ¼
                in_content = table.xpath('./ancestor-or-self::div[@id="divContent"] | ./ancestor-or-self::*[@id="mainContent"]').get()
                if not in_content:
                    continue
                
                # æ’é™¤å°èˆªå€åŸŸ
                in_nav = table.xpath('./ancestor::nav | ./ancestor::header | ./ancestor::*[contains(@class, "nav")] | ./ancestor::*[contains(@class, "menu")]').get()
                if in_nav:
                    continue
                
                # æå–è¡¨é ­
                headers = table.xpath('.//th//text()').getall()
                if headers:
                    header_text = ' | '.join([h.strip() for h in headers if h.strip()])
                    if header_text:
                        content_parts.append(header_text)
                
                # æå–è¡¨æ ¼è¡Œ
                rows = table.xpath('.//tr')
                for row in rows:
                    # æå–æ¯å€‹å–®å…ƒæ ¼çš„æ‰€æœ‰æ–‡å­—ï¼ˆåŒ…æ‹¬ <br> åˆ†éš”çš„å…§å®¹ï¼‰
                    cells = row.xpath('.//td')
                    row_data = []
                    for cell in cells:
                        # ä½¿ç”¨ XPath æå–æ‰€æœ‰æ–‡å­—ç¯€é»ï¼ˆåŒ…æ‹¬ <br> å¾Œçš„æ–‡å­—ï¼‰
                        cell_texts = cell.xpath('.//text()').getall()
                        # åˆä½µä¸¦æ¸…ç†
                        cell_text = ' '.join([t.strip() for t in cell_texts if t.strip()])
                        if cell_text:
                            row_data.append(cell_text)
                    if row_data:
                        content_parts.append(' | '.join(row_data))
        
        # 5. æŠ“å–åˆ—è¡¨å…§å®¹ï¼ˆåŒ…æ‹¬é …ç›®åˆ—è¡¨ï¼‰ï¼Œä½†åš´æ ¼æ’é™¤å°èˆªå€åŸŸ
        list_items = response.css('ul li, ol li, dl dt, dl dd')
        for item in list_items:
            # åš´æ ¼æ’é™¤å°èˆªå€åŸŸ
            in_nav = item.xpath('./ancestor::nav | ./ancestor::header | ./ancestor::*[contains(@class, "nav")] | ./ancestor::*[contains(@class, "menu")] | ./ancestor::*[contains(@class, "mega-menu")] | ./ancestor::*[contains(@class, "function_link")]').get()
            if in_nav:
                continue
            
            # åªæå–åœ¨ #divContent æˆ– #mainContent ä¸­çš„å…§å®¹
            in_content = item.xpath('./ancestor-or-self::div[@id="divContent"] | ./ancestor-or-self::*[@id="mainContent"]').get()
            if not in_content:
                continue
            
            item_text = ' '.join(item.css('::text').getall())
            if item_text and len(item_text.strip()) > 5:
                content_parts.append(item_text.strip())
        
        # 6. æŠ“å–æ‰€æœ‰æ®µè½ï¼ˆä½†éæ¿¾å¤ªçŸ­çš„ï¼Œä¸”æ’é™¤å·²ç¶“åœ¨ #divContent ä¸­è™•ç†éçš„ï¼‰
        paragraphs = response.css('p')
        for para in paragraphs:
            # åš´æ ¼æ’é™¤å°èˆªå€åŸŸ
            in_nav = para.xpath('./ancestor::nav | ./ancestor::header | ./ancestor::*[contains(@class, "nav")] | ./ancestor::*[contains(@class, "menu")] | ./ancestor::*[contains(@class, "mega-menu")] | ./ancestor::*[contains(@class, "function_link")]').get()
            if in_nav:
                continue
            
            # åªæå–åœ¨ #divContent æˆ– #mainContent ä¸­çš„å…§å®¹
            in_content = para.xpath('./ancestor-or-self::div[@id="divContent"] | ./ancestor-or-self::*[@id="mainContent"]').get()
            if not in_content:
                continue
            
            para_text = ' '.join(para.css('::text').getall())
            if para_text and len(para_text.strip()) > 10:
                content_parts.append(para_text.strip())
        
        # 7. æå– div ä¸­çš„æ–‡å­—å…§å®¹ï¼ˆåš´æ ¼æ’é™¤å°èˆªå’Œé è…³ï¼‰
        # åªå¾ #mainContent æˆ– #divContent ä¸­æå–
        main_content = response.css('#mainContent, #divContent')
        if main_content:
            for content_div in main_content:
                # æ’é™¤å°èˆªå€åŸŸ
                in_nav = content_div.xpath('./ancestor::nav | ./ancestor::header | ./ancestor::*[contains(@class, "nav")] | ./ancestor::*[contains(@class, "menu")] | ./ancestor::*[contains(@class, "mega-menu")]').get()
                if in_nav:
                    continue
                
                # æå–æ–‡å­—ï¼Œä½†æ’é™¤å­å…ƒç´ ä¸­çš„å°èˆª
                text_nodes = content_div.xpath('.//text()[not(ancestor::nav) and not(ancestor::header) and not(ancestor::*[contains(@class, "nav")]) and not(ancestor::*[contains(@class, "menu")]) and not(ancestor::*[contains(@class, "mega-menu")])]').getall()
                if text_nodes:
                    text_content = ' '.join([t.strip() for t in text_nodes if t.strip() and len(t.strip()) > 3])
                    if len(text_content) > 20:
                        content_parts.append(text_content)
        
        # æ¸…ç†å’Œçµ„åˆå…§å®¹ï¼ˆå»é‡ä¸¦ä¿æŒé †åºï¼‰
        # ä½¿ç”¨æ›´æ™ºèƒ½çš„å»é‡ï¼šä¿ç•™è¼ƒé•·çš„å…§å®¹ï¼Œé¿å…çŸ­å…§å®¹è¦†è“‹é•·å…§å®¹
        seen = set()
        unique_content_parts = []
        # å…ˆæŒ‰é•·åº¦æ’åºï¼Œé•·å…§å®¹å„ªå…ˆ
        sorted_parts = sorted(content_parts, key=lambda x: len(x.strip()), reverse=True)
        
        for part in sorted_parts:
            part_clean = part.strip()
            if not part_clean or len(part_clean) <= 3:
                continue
            
            # æª¢æŸ¥æ˜¯å¦èˆ‡å·²æœ‰å…§å®¹é‡è¤‡ï¼ˆå…è¨±éƒ¨åˆ†é‡ç–Šï¼Œä½†ä¸å…è¨±å®Œå…¨é‡è¤‡ï¼‰
            is_duplicate = False
            for existing in unique_content_parts:
                # å¦‚æœæ–°å…§å®¹æ˜¯å·²æœ‰å…§å®¹çš„å­ä¸²ï¼Œè·³é
                if part_clean in existing and len(part_clean) < len(existing) * 0.8:
                    is_duplicate = True
                    break
                # å¦‚æœå·²æœ‰å…§å®¹æ˜¯æ–°å…§å®¹çš„å­ä¸²ï¼Œæ›¿æ›å·²æœ‰å…§å®¹
                if existing in part_clean and len(existing) < len(part_clean) * 0.8:
                    unique_content_parts.remove(existing)
                    break
            
            if not is_duplicate and part_clean not in seen:
                seen.add(part_clean)
                unique_content_parts.append(part_clean)
        
        # çµ„åˆå…§å®¹ï¼Œä½¿ç”¨æ›è¡Œåˆ†éš”ä¸åŒé¡å‹çš„å…§å®¹
        content = "\n".join(unique_content_parts)
        
        # éæ¿¾æ‰ JavaScript éŒ¯èª¤è¨Šæ¯å’Œç„¡ç”¨æ–‡å­—
        filter_patterns = [
            'æ‚¨çš„ç€è¦½å™¨ä¸æ”¯æ´JavaScript',
            'è«‹é–‹å•Ÿç€è¦½å™¨JavaScript',
            'INNOVATIONG A BETTER FUTURE',
            'JavaScript',
            'function',
            'var ',
            'document.',
            'window.',
        ]
        
        for pattern in filter_patterns:
            if pattern in content:
                # ç§»é™¤åŒ…å«é€™äº›æ¨¡å¼çš„å¥å­
                sentences = content.split('ã€‚')
                content = 'ã€‚'.join([s for s in sentences if pattern not in s])
        
        # æ¸…ç†å¤šé¤˜çš„ç©ºç™½ï¼ˆä½†ä¿ç•™æ›è¡Œï¼‰
        lines = content.split('\n')
        cleaned_lines = [re.sub(r'\s+', ' ', line).strip() for line in lines if line.strip()]
        content = '\n'.join(cleaned_lines)
        
        # éæ¿¾æ¢ä»¶ï¼šè‡³å°‘æœ‰ 30 å€‹å­—å…ƒï¼ˆé™ä½é–€æª»ä»¥åŒ…å«è¡¨æ ¼å…§å®¹ï¼‰
        # ä½†ä¹Ÿè¦ç¢ºä¿ä¸æ˜¯åªæœ‰é‡è¤‡çš„ç°¡ä»‹
        if len(content) >= 30:
            # æª¢æŸ¥æ˜¯å¦åªæ˜¯é‡è¤‡çš„ç°¡ä»‹ï¼ˆå¦‚æœå…§å®¹å¤ªçŸ­ä¸”èˆ‡æ¨™é¡Œç›¸ä¼¼ï¼Œå¯èƒ½æ˜¯é‡è¤‡ï¼‰
            content_words = set(content.split())
            title_words = set(title.split())
            # å¦‚æœå…§å®¹å’Œæ¨™é¡Œé‡ç–Šåº¦å¤ªé«˜ï¼Œå¯èƒ½åªæ˜¯å°èˆªï¼Œéœ€è¦æ›´å¤šå…§å®¹
            # ä½†æˆ‘å€‘ä»ç„¶ä¿å­˜ï¼Œå› ç‚ºå¯èƒ½åŒ…å«ç›¸é—œé …ç›®åˆ—è¡¨
            
            # å¦‚æœæ¨™é¡Œç‚ºç©ºï¼Œå˜—è©¦å¾å…§å®¹ä¸­æå–
            if not title or title == "å·¥ç ”é™¢é é¢":
                # å˜—è©¦å¾å…§å®¹ç¬¬ä¸€è¡Œæå–æ¨™é¡Œ
                first_line = content.split('\n')[0][:50] if '\n' in content else content[:50]
                if len(first_line.strip()) > 5:
                    title = first_line.strip()
            
                    yield {
                        'source': 'ITRI_Official',
                        'title': title,
                        'url': response.url,
                        'content': content,
                        'hierarchy': f"ITRI > Official > {title}",
                        'depth': 1,
                        'language': 'zh-tw',
                        'crawled_at': datetime.now().isoformat()
                    }
                    
                    # å¾ç•¶å‰é é¢ä¸­æå–é€£çµï¼Œç¹¼çºŒçˆ¬å–ï¼ˆä½†é™åˆ¶æ·±åº¦ï¼‰
                    if len(self.visited_urls) < self.max_pages:
                        # å¾ç•¶å‰é é¢æå–æ‰€æœ‰ ListStyle é€£çµï¼ˆåŒ…æ‹¬ mega-menu ä¸­çš„ï¼‰
                        page_links = response.xpath(
                            '//a[contains(@href, "ListStyle")]/@href | '
                            '//*[contains(@class, "mega-menu")]//a[contains(@href, "ListStyle")]/@href | '
                            '//nav//a[contains(@href, "ListStyle")]/@href'
                        ).getall()
                        
                        # å»é‡
                        unique_links = list(set(page_links))
                        
                        for link in unique_links[:20]:  # æ¯å€‹é é¢æœ€å¤šè·Ÿéš¨ 20 å€‹é€£çµ
                            if not link:
                                continue
                            full_url = response.urljoin(link)
                            # æ¨™æº–åŒ– URL
                            url_normalized = full_url.split('#')[0]
                            # é©—è­‰ URL æ˜¯å¦æœ‰æ•ˆ
                            if self._is_valid_url(full_url) and url_normalized not in self.visited_urls:
                                self.visited_urls.add(url_normalized)
                                yield response.follow(link, self.parse_detail, dont_filter=True)


# --- åŸ·è¡Œçˆ¬èŸ² ---
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ•·ï¸  ITRI Data Crawler - Step 1: Data Collection")
    print("=" * 60)
    print(f"ğŸ“ è¼¸å‡ºæª”æ¡ˆ: {OUTPUT_FILE}")
    print(f"ğŸ“… é–‹å§‹æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    process = CrawlerProcess(settings={
        'FEEDS': {
            OUTPUT_FILE: {
                'format': 'json',
                'encoding': 'utf8',
                'indent': 4,
                'overwrite': True
            },
        },
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'ROBOTSTXT_OBEY': True,   # éµå®ˆ robots.txtï¼ˆç¶­åŸºç™¾ç§‘å…è¨±åˆç†çš„çˆ¬å–ï¼‰
        'DEPTH_LIMIT': 2,           # Wiki BFS é™åˆ¶æ·±åº¦ 2
        'CONCURRENT_REQUESTS': 16,  # åŠ å¿«çˆ¬å–é€Ÿåº¦
        'DOWNLOAD_DELAY': 1,        # ç¦®è²Œå»¶é²
        'LOG_LEVEL': 'INFO',
        'DUPEFILTER_CLASS': 'scrapy.dupefilters.RFPDupeFilter',
    })

    print("\nğŸš€ é–‹å§‹çˆ¬å–ç¶­åŸºç™¾ç§‘...")
    process.crawl(ItriWikiSpider)
    
    print("ğŸš€ é–‹å§‹çˆ¬å–å·¥ç ”é™¢å®˜ç¶²...")
    # æª¢æŸ¥æ˜¯å¦è¦ä½¿ç”¨ Seleniumï¼ˆé€šéç’°å¢ƒè®Šæ•¸æˆ–å‘½ä»¤è¡Œåƒæ•¸ï¼‰
    use_selenium = os.getenv('USE_SELENIUM', 'false').lower() == 'true'
    if use_selenium and not SELENIUM_AVAILABLE:
        print("âš ï¸  ç’°å¢ƒè®Šæ•¸ USE_SELENIUM=trueï¼Œä½† Selenium æœªå®‰è£")
        print("   è«‹åŸ·è¡Œ: pip install selenium")
        use_selenium = False
    
    process.crawl(ItriOfficialSpider, use_selenium=use_selenium)
    
    print("\nâ³ çˆ¬èŸ²åŸ·è¡Œä¸­ï¼Œè«‹ç¨å€™...\n")
    process.start()
    
    # æª¢æŸ¥è¼¸å‡ºæª”æ¡ˆ
    if os.path.exists(OUTPUT_FILE):
        import json
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"\nâœ… çˆ¬èŸ²å®Œæˆï¼")
        print(f"ğŸ“Š å…±çˆ¬å– {len(data)} ç­†è³‡æ–™")
        print(f"ğŸ’¾ è³‡æ–™å·²å„²å­˜è‡³: {OUTPUT_FILE}")
    else:
        print("\nâš ï¸  è­¦å‘Šï¼šæœªæ‰¾åˆ°è¼¸å‡ºæª”æ¡ˆï¼Œçˆ¬èŸ²å¯èƒ½æœªæˆåŠŸåŸ·è¡Œ")

