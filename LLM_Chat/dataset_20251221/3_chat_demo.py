#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ITRI Chat Demo - Step 3: Chat with RAG
ä½¿ç”¨å»ºç½®å¥½çš„ RAG è³‡æ–™åº«é€²è¡Œå°è©±æ¸¬è©¦
"""

import time
from datetime import datetime

try:
    from langchain_community.vectorstores import Chroma
    from langchain_community.embeddings import OllamaEmbeddings
    from langchain_community.chat_models import ChatOllama
    from langchain.prompts import ChatPromptTemplate
    from langchain_core.runnables import RunnablePassthrough
    from langchain_core.output_parsers import StrOutputParser
except ImportError as e:
    print("âŒ ç¼ºå°‘å¿…è¦çš„å¥—ä»¶ï¼Œè«‹å…ˆå®‰è£ï¼š")
    print("   pip install langchain langchain-community chromadb")
    print(f"\néŒ¯èª¤è©³æƒ…: {e}")
    exit(1)

# --- è¨­å®š ---
CHROMA_PATH = "./chroma_db_itri"
EMBED_MODEL = "bge-m3"  # å¿…é ˆèˆ‡å»ºç½®è³‡æ–™åº«æ™‚ä½¿ç”¨çš„æ¨¡å‹ç›¸åŒ
LLM_MODEL = "llama3.1:70b-instruct-q4-0"    # æˆ–ä½¿ç”¨å…¶ä»–æ¨¡å‹å¦‚ "mistral", "gemma"
OLLAMA_BASE_URL = "http://localhost:11435"

# RAG åƒæ•¸
TOP_K = 5  # æª¢ç´¢å‰ K å€‹æœ€ç›¸é—œçš„ç‰‡æ®µ


def format_docs(docs):
    """å°‡æª¢ç´¢åˆ°çš„å¤šå€‹å€å¡Šçµ„åˆæˆä¸€æ®µæ–‡å­—ï¼Œä¸¦é™„ä¸Šä¾†æº"""
    formatted_content = []
    for i, doc in enumerate(docs):
        source_info = f"[ä¾†æº {i+1}: {doc.metadata.get('hierarchy', 'Unknown')}]"
        title_info = f"æ¨™é¡Œ: {doc.metadata.get('title', 'Untitled')}"
        formatted_content.append(f"{source_info}\n{title_info}\n{doc.page_content}")
    return "\n\n".join(formatted_content)


def check_ollama_connection():
    """æª¢æŸ¥ Ollama é€£æ¥"""
    try:
        import requests
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            model_names = [m.get('name', '') for m in models]
            
            # æª¢æŸ¥å¿…è¦çš„æ¨¡å‹
            missing_models = []
            if EMBED_MODEL not in model_names:
                missing_models.append(EMBED_MODEL)
            if LLM_MODEL not in model_names:
                missing_models.append(LLM_MODEL)
            
            if missing_models:
                print(f"âš ï¸  è­¦å‘Š: ä»¥ä¸‹æ¨¡å‹å°šæœªä¸‹è¼‰:")
                for model in missing_models:
                    print(f"   - {model}")
                print(f"\nè«‹åŸ·è¡Œä»¥ä¸‹æŒ‡ä»¤ä¸‹è¼‰æ¨¡å‹:")
                for model in missing_models:
                    print(f"   ollama pull {model}")
                return False
            
            return True
        else:
            print(f"âŒ ç„¡æ³•é€£æ¥åˆ° Ollama ({OLLAMA_BASE_URL})")
            return False
    except Exception as e:
        print(f"âŒ ç„¡æ³•é€£æ¥åˆ° Ollama: {e}")
        print(f"   è«‹ç¢ºèª Ollama å·²å•Ÿå‹•ä¸¦é‹è¡Œåœ¨ {OLLAMA_BASE_URL}")
        return False


def check_database():
    """æª¢æŸ¥è³‡æ–™åº«æ˜¯å¦å­˜åœ¨"""
    import os
    if not os.path.exists(CHROMA_PATH):
        print(f"âŒ æ‰¾ä¸åˆ°è³‡æ–™åº«: {CHROMA_PATH}")
        print("   è«‹å…ˆåŸ·è¡Œ 2_build_rag.py å»ºç«‹è³‡æ–™åº«")
        return False
    return True


def main():
    """ä¸»å‡½æ•¸"""
    print("=" * 60)
    print("ğŸ¤– ITRI AI å°è¦½å“¡ - RAG Chat Demo")
    print("=" * 60)
    
    # æª¢æŸ¥è³‡æ–™åº«
    if not check_database():
        return
    
    # æª¢æŸ¥ Ollama é€£æ¥
    if not check_ollama_connection():
        return
    
    print(f"\nğŸš€ å•Ÿå‹•å·¥ç ”é™¢ AI å°è¦½å“¡...")
    print(f"   ğŸ“š è³‡æ–™åº«: {CHROMA_PATH}")
    print(f"   ğŸ¤– Embedding Model: {EMBED_MODEL}")
    print(f"   ğŸ’¬ LLM Model: {LLM_MODEL}")
    print(f"   ğŸ” æª¢ç´¢æ•¸é‡: Top {TOP_K}")
    
    try:
        # 1. é€£æ¥è³‡æ–™åº«
        print(f"\n1ï¸âƒ£  é€£æ¥å‘é‡è³‡æ–™åº«...")
        embedding_func = OllamaEmbeddings(
            model=EMBED_MODEL,
            base_url=OLLAMA_BASE_URL
        )
        
        db = Chroma(
            persist_directory=CHROMA_PATH,
            embedding_function=embedding_func
        )
        
        # æª¢æŸ¥è³‡æ–™åº«ä¸­çš„æ–‡ä»¶æ•¸é‡
        collection_count = db._collection.count()
        print(f"   âœ… è³‡æ–™åº«é€£æ¥æˆåŠŸ")
        print(f"   ğŸ“Š è³‡æ–™åº«ä¸­çš„å‘é‡æ•¸é‡: {collection_count}")
        
        if collection_count == 0:
            print("   âš ï¸  è­¦å‘Š: è³‡æ–™åº«ç‚ºç©ºï¼Œè«‹é‡æ–°å»ºç½®è³‡æ–™åº«")
            return
        
        # å»ºç«‹æª¢ç´¢å™¨
        retriever = db.as_retriever(
            search_type="similarity",
            search_kwargs={"k": TOP_K}
        )
        print(f"   âœ… æª¢ç´¢å™¨å»ºç«‹å®Œæˆ")
        
        # 2. è¨­å®š LLM
        print(f"\n2ï¸âƒ£  åˆå§‹åŒ– LLM...")
        llm = ChatOllama(
            model=LLM_MODEL,
            base_url=OLLAMA_BASE_URL,
            temperature=0.7
        )
        print(f"   âœ… LLM åˆå§‹åŒ–å®Œæˆ")
        
        # 3. è¨­è¨ˆ Prompt
        print(f"\n3ï¸âƒ£  è¨­å®š Prompt æ¨¡æ¿...")
        template = """ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ã€Œå·¥ç ”é™¢ (ITRI) å°è¦½å“¡ã€ã€‚
è«‹æ ¹æ“šä»¥ä¸‹æä¾›çš„èƒŒæ™¯çŸ¥è­˜ (Context) ä¾†å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚

é‡è¦åŸå‰‡ï¼š
- å¦‚æœçŸ¥è­˜åº«ä¸­æ²’æœ‰ç­”æ¡ˆï¼Œè«‹èª å¯¦èªªä¸çŸ¥é“ï¼Œä¸è¦ç·¨é€ äº‹å¯¦
- å›ç­”æ™‚è«‹èªæ°£è¦ªåˆ‡ã€å°ˆæ¥­ï¼Œä¸¦ç›¡é‡å¼•ç”¨ä¾†æº
- å„ªå…ˆä½¿ç”¨èƒŒæ™¯çŸ¥è­˜ä¸­çš„è³‡è¨Šï¼Œä¸è¦ä½¿ç”¨å¤–éƒ¨çŸ¥è­˜
- å¦‚æœè³‡è¨Šä¸ç¢ºå®šæˆ–éæ™‚ï¼Œè«‹æ˜ç¢ºèªªæ˜

èƒŒæ™¯çŸ¥è­˜ Context:
{context}

ä½¿ç”¨è€…å•é¡Œ Question: 
{question}

å›ç­” Answer (è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”):"""
        
        prompt = ChatPromptTemplate.from_template(template)
        print(f"   âœ… Prompt æ¨¡æ¿è¨­å®šå®Œæˆ")
        
        # 4. å»ºç«‹ Chain
        print(f"\n4ï¸âƒ£  å»ºç«‹ RAG Chain...")
        rag_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        print(f"   âœ… RAG Chain å»ºç«‹å®Œæˆ")
        
        print("\n" + "=" * 60)
        print("âœ… å°è¦½å“¡æº–å‚™å°±ç·’ï¼")
        print("=" * 60)
        print("ğŸ’¡ æç¤º: è¼¸å…¥ 'exit' æˆ– 'quit' é›¢é–‹")
        print("=" * 60)
        
        # 5. äº’å‹•è¿´åœˆ
        conversation_count = 0
        while True:
            print()
            query = input("â“ è«‹è¼¸å…¥å•é¡Œ (e.g., å·¥ç ”é™¢çš„2030æŠ€è¡“ç­–ç•¥æ˜¯ä»€éº¼?): ").strip()
            
            if query.lower() in ['exit', 'quit', 'é›¢é–‹', 'é€€å‡º']:
                print("\nğŸ‘‹ æ„Ÿè¬ä½¿ç”¨ï¼Œå†è¦‹ï¼")
                break
            
            if not query:
                print("âš ï¸  è«‹è¼¸å…¥æœ‰æ•ˆçš„å•é¡Œ")
                continue
            
            conversation_count += 1
            print(f"\nğŸ¤– æ€è€ƒä¸­... (å•é¡Œ #{conversation_count})")
            start_time = time.time()
            
            try:
                # åŸ·è¡Œ RAG
                response = rag_chain.invoke(query)
                
                elapsed_time = time.time() - start_time
                
                print(f"\n{'='*60}")
                print("ğŸ“ å›ç­”:")
                print(f"{'='*60}")
                print(response)
                print(f"{'='*60}")
                print(f"â±ï¸  è€—æ™‚: {elapsed_time:.2f} ç§’")
                print(f"{'='*60}")
                
            except Exception as e:
                print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
                import traceback
                traceback.print_exc()
                print("   è«‹æª¢æŸ¥ Ollama æ˜¯å¦æ­£å¸¸é‹è¡Œï¼Œæˆ–æ¨¡å‹æ˜¯å¦å·²æ­£ç¢ºä¸‹è¼‰")
        
    except Exception as e:
        print(f"\nâŒ åˆå§‹åŒ–å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()







